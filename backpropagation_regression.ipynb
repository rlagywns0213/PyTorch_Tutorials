{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lecture06.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMdklUJ/+N394jxKdrmm7ko",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rlagywns0213/PyTorch_Tutorials/blob/main/backpropagation_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqiGbf3bIqQ5"
      },
      "source": [
        "# Data and Variable\n",
        "\n",
        "- 간단한 forward pass 함수로부터 gradient descent 이루어지는 과정\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8Kqj6qoIt5g",
        "outputId": "4f62e6e7-12c3-490d-ae18-a709f1055189"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "w = torch.tensor([1.0], requires_grad=True)\n",
        "\n",
        "#forward pass 함수\n",
        "def forward(x):\n",
        "    return x * w\n",
        "\n",
        "def loss(x,y):\n",
        "    y_pred = forward(x)\n",
        "    return (y_pred-y) ** 2 \n",
        "\n",
        "for epoch in range(10):\n",
        "    for x_val, y_val in zip(x_data, y_data):\n",
        "        l = loss(x_val, y_val) #loss 함수를 실행시키면, forward까지 이루어져서 loss저장\n",
        "        l.backward() #gradient를 직접 구할필요없이, 모든 변수마다 gradient 갱신하는 back propation 진행해줌!!(매우 유용)\n",
        "        print(\"\\tgrad: \", x_val, y_val, w.grad.item())\n",
        "        w.data = w.data - 0.01 * w.grad.item() #gradient descent를 수행하는 부분 (갱신)\n",
        "        w.grad.data.zero_() # gradient인 미분값을 다시 0으로 세팅해주어야함 (data는 계속 업데이트해야되니 초기화하는게 아니고!)\n",
        "    print(f\"Epoch: {epoch} | Loss: {l.item()}\")\n",
        "\n",
        "print(\"Prediction (after training)\", 4, forward(4).item())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tgrad:  1.0 2.0 -2.0\n",
            "\tgrad:  2.0 4.0 -7.840000152587891\n",
            "\tgrad:  3.0 6.0 -16.228801727294922\n",
            "Epoch: 0 | Loss: 7.315943717956543\n",
            "\tgrad:  1.0 2.0 -1.478623867034912\n",
            "\tgrad:  2.0 4.0 -5.796205520629883\n",
            "\tgrad:  3.0 6.0 -11.998146057128906\n",
            "Epoch: 1 | Loss: 3.9987640380859375\n",
            "\tgrad:  1.0 2.0 -1.0931644439697266\n",
            "\tgrad:  2.0 4.0 -4.285204887390137\n",
            "\tgrad:  3.0 6.0 -8.870372772216797\n",
            "Epoch: 2 | Loss: 2.1856532096862793\n",
            "\tgrad:  1.0 2.0 -0.8081896305084229\n",
            "\tgrad:  2.0 4.0 -3.1681032180786133\n",
            "\tgrad:  3.0 6.0 -6.557973861694336\n",
            "Epoch: 3 | Loss: 1.1946394443511963\n",
            "\tgrad:  1.0 2.0 -0.5975041389465332\n",
            "\tgrad:  2.0 4.0 -2.3422164916992188\n",
            "\tgrad:  3.0 6.0 -4.848389625549316\n",
            "Epoch: 4 | Loss: 0.6529689431190491\n",
            "\tgrad:  1.0 2.0 -0.4417421817779541\n",
            "\tgrad:  2.0 4.0 -1.7316293716430664\n",
            "\tgrad:  3.0 6.0 -3.58447265625\n",
            "Epoch: 5 | Loss: 0.35690122842788696\n",
            "\tgrad:  1.0 2.0 -0.3265852928161621\n",
            "\tgrad:  2.0 4.0 -1.2802143096923828\n",
            "\tgrad:  3.0 6.0 -2.650045394897461\n",
            "Epoch: 6 | Loss: 0.195076122879982\n",
            "\tgrad:  1.0 2.0 -0.24144840240478516\n",
            "\tgrad:  2.0 4.0 -0.9464778900146484\n",
            "\tgrad:  3.0 6.0 -1.9592113494873047\n",
            "Epoch: 7 | Loss: 0.10662525147199631\n",
            "\tgrad:  1.0 2.0 -0.17850565910339355\n",
            "\tgrad:  2.0 4.0 -0.699742317199707\n",
            "\tgrad:  3.0 6.0 -1.4484672546386719\n",
            "Epoch: 8 | Loss: 0.0582793727517128\n",
            "\tgrad:  1.0 2.0 -0.1319713592529297\n",
            "\tgrad:  2.0 4.0 -0.5173273086547852\n",
            "\tgrad:  3.0 6.0 -1.070866584777832\n",
            "Epoch: 9 | Loss: 0.03185431286692619\n",
            "Prediction (after training) 4 7.804864406585693\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ8x_cYkQkcA"
      },
      "source": [
        "# PyTorch Design Pattern\n",
        "\n",
        "1. nn.Module을 상속 받아 모델을 design한다.\n",
        "\n",
        "2. 학습하기 위한 ㅣoss, optimizer function 정의한다.\n",
        "\n",
        "3. Training cycle을 정의\n",
        "\n",
        "## linear regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaCpcOX4MfkV",
        "outputId": "0f06b20c-c93e-4874-c55e-f9c4f1c1e80e"
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor\n",
        "\n",
        "x_data = tensor([[1.0], [2.0], [3.0]])\n",
        "y_data = tensor([[2.0], [4.0], [6.0]])\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__() #super : 상속받은 module에서의 생성자를 호출\n",
        "        self.linear = torch.nn.Linear(1,1) # 모델 생성 => y = wx + b , 단순 선형   #bias default로 true로 들어가있음\n",
        "    \n",
        "    def forward(self, x):  \n",
        "        y_pred = self.linear(x) #생성자에서 만들어놓은 객체를 사용하겠다.\n",
        "        return y_pred\n",
        "\n",
        "model = Model() #클래스 외부에서 instance 만들어줌\n",
        "\n",
        "criterion = torch.nn.MSELoss(reduction = 'sum') #mse 를 직접만들 필요없이 파이토치에서 제공   #reduction = \"mean\" : 개수를 나눠주는것 (평균값:일반적)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "#training cycle 구축\n",
        "for epoch in range(500):\n",
        "    # 1) Forward pass : #forward 함수가 실행됨 (앞에서는 for문 두번이였는데 연산량 줄여줌)\n",
        "    y_pred = model(x_data) \n",
        "\n",
        "    # 2) Compute loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch: {epoch} | Loss: {loss.item()}')\n",
        "\n",
        "    # backward (local gradient를 구한다) -> update (SGD를 수행)\n",
        "    optimizer.zero_grad() #현재 가중치에 적재되어있던 gradient를 0으로 만들어주는 것\n",
        "    loss.backward() #back propagation 진행\n",
        "    optimizer.step() #업데이트를 해줌\n",
        "\n",
        "hour_var = tensor([[4.0]])\n",
        "y_pred = model(hour_var)\n",
        "print(\"Prediction (after training)\", 4, model(hour_var).item())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 | Loss: 115.25199890136719\n",
            "Epoch: 1 | Loss: 51.33973693847656\n",
            "Epoch: 2 | Loss: 22.887344360351562\n",
            "Epoch: 3 | Loss: 10.220685005187988\n",
            "Epoch: 4 | Loss: 4.5813889503479\n",
            "Epoch: 5 | Loss: 2.070484161376953\n",
            "Epoch: 6 | Loss: 0.9522541165351868\n",
            "Epoch: 7 | Loss: 0.45401206612586975\n",
            "Epoch: 8 | Loss: 0.2317754179239273\n",
            "Epoch: 9 | Loss: 0.13241642713546753\n",
            "Epoch: 10 | Loss: 0.0877639427781105\n",
            "Epoch: 11 | Loss: 0.06747207790613174\n",
            "Epoch: 12 | Loss: 0.05803045257925987\n",
            "Epoch: 13 | Loss: 0.0534249022603035\n",
            "Epoch: 14 | Loss: 0.05097828060388565\n",
            "Epoch: 15 | Loss: 0.04949808493256569\n",
            "Epoch: 16 | Loss: 0.048453979194164276\n",
            "Epoch: 17 | Loss: 0.047609567642211914\n",
            "Epoch: 18 | Loss: 0.046859391033649445\n",
            "Epoch: 19 | Loss: 0.0461566261947155\n",
            "Epoch: 20 | Loss: 0.04548022896051407\n",
            "Epoch: 21 | Loss: 0.04482072591781616\n",
            "Epoch: 22 | Loss: 0.04417405277490616\n",
            "Epoch: 23 | Loss: 0.04353804141283035\n",
            "Epoch: 24 | Loss: 0.042911823838949203\n",
            "Epoch: 25 | Loss: 0.042294956743717194\n",
            "Epoch: 26 | Loss: 0.04168691858649254\n",
            "Epoch: 27 | Loss: 0.041087664663791656\n",
            "Epoch: 28 | Loss: 0.040497247129678726\n",
            "Epoch: 29 | Loss: 0.039915185421705246\n",
            "Epoch: 30 | Loss: 0.03934159502387047\n",
            "Epoch: 31 | Loss: 0.03877612203359604\n",
            "Epoch: 32 | Loss: 0.03821887448430061\n",
            "Epoch: 33 | Loss: 0.037669599056243896\n",
            "Epoch: 34 | Loss: 0.0371282622218132\n",
            "Epoch: 35 | Loss: 0.03659462183713913\n",
            "Epoch: 36 | Loss: 0.036068692803382874\n",
            "Epoch: 37 | Loss: 0.03555039316415787\n",
            "Epoch: 38 | Loss: 0.03503942862153053\n",
            "Epoch: 39 | Loss: 0.03453580662608147\n",
            "Epoch: 40 | Loss: 0.034039635211229324\n",
            "Epoch: 41 | Loss: 0.03355032205581665\n",
            "Epoch: 42 | Loss: 0.03306811675429344\n",
            "Epoch: 43 | Loss: 0.03259291872382164\n",
            "Epoch: 44 | Loss: 0.03212451562285423\n",
            "Epoch: 45 | Loss: 0.031662844121456146\n",
            "Epoch: 46 | Loss: 0.031207842752337456\n",
            "Epoch: 47 | Loss: 0.0307592935860157\n",
            "Epoch: 48 | Loss: 0.030317232012748718\n",
            "Epoch: 49 | Loss: 0.029881548136472702\n",
            "Epoch: 50 | Loss: 0.02945202961564064\n",
            "Epoch: 51 | Loss: 0.02902884967625141\n",
            "Epoch: 52 | Loss: 0.028611652553081512\n",
            "Epoch: 53 | Loss: 0.02820037491619587\n",
            "Epoch: 54 | Loss: 0.027795149013400078\n",
            "Epoch: 55 | Loss: 0.027395712211728096\n",
            "Epoch: 56 | Loss: 0.02700202539563179\n",
            "Epoch: 57 | Loss: 0.026613891124725342\n",
            "Epoch: 58 | Loss: 0.026231447234749794\n",
            "Epoch: 59 | Loss: 0.02585441619157791\n",
            "Epoch: 60 | Loss: 0.02548287995159626\n",
            "Epoch: 61 | Loss: 0.025116678327322006\n",
            "Epoch: 62 | Loss: 0.02475564368069172\n",
            "Epoch: 63 | Loss: 0.02439993806183338\n",
            "Epoch: 64 | Loss: 0.024049222469329834\n",
            "Epoch: 65 | Loss: 0.023703590035438538\n",
            "Epoch: 66 | Loss: 0.02336292900145054\n",
            "Epoch: 67 | Loss: 0.02302715741097927\n",
            "Epoch: 68 | Loss: 0.02269626408815384\n",
            "Epoch: 69 | Loss: 0.022370075806975365\n",
            "Epoch: 70 | Loss: 0.02204858884215355\n",
            "Epoch: 71 | Loss: 0.021731676533818245\n",
            "Epoch: 72 | Loss: 0.021419387310743332\n",
            "Epoch: 73 | Loss: 0.0211115051060915\n",
            "Epoch: 74 | Loss: 0.020808199420571327\n",
            "Epoch: 75 | Loss: 0.02050912007689476\n",
            "Epoch: 76 | Loss: 0.020214298740029335\n",
            "Epoch: 77 | Loss: 0.01992383599281311\n",
            "Epoch: 78 | Loss: 0.019637543708086014\n",
            "Epoch: 79 | Loss: 0.019355323165655136\n",
            "Epoch: 80 | Loss: 0.019077101722359657\n",
            "Epoch: 81 | Loss: 0.018802888691425323\n",
            "Epoch: 82 | Loss: 0.018532704561948776\n",
            "Epoch: 83 | Loss: 0.01826636679470539\n",
            "Epoch: 84 | Loss: 0.018003832548856735\n",
            "Epoch: 85 | Loss: 0.01774510182440281\n",
            "Epoch: 86 | Loss: 0.01749008148908615\n",
            "Epoch: 87 | Loss: 0.017238778993487358\n",
            "Epoch: 88 | Loss: 0.01699099875986576\n",
            "Epoch: 89 | Loss: 0.016746819019317627\n",
            "Epoch: 90 | Loss: 0.016506146639585495\n",
            "Epoch: 91 | Loss: 0.016268860548734665\n",
            "Epoch: 92 | Loss: 0.016035106033086777\n",
            "Epoch: 93 | Loss: 0.015804577618837357\n",
            "Epoch: 94 | Loss: 0.015577461570501328\n",
            "Epoch: 95 | Loss: 0.015353601425886154\n",
            "Epoch: 96 | Loss: 0.01513296365737915\n",
            "Epoch: 97 | Loss: 0.014915463514626026\n",
            "Epoch: 98 | Loss: 0.01470110286027193\n",
            "Epoch: 99 | Loss: 0.014489840716123581\n",
            "Epoch: 100 | Loss: 0.014281588606536388\n",
            "Epoch: 101 | Loss: 0.014076346531510353\n",
            "Epoch: 102 | Loss: 0.013874068856239319\n",
            "Epoch: 103 | Loss: 0.013674670830368996\n",
            "Epoch: 104 | Loss: 0.013478105887770653\n",
            "Epoch: 105 | Loss: 0.013284377753734589\n",
            "Epoch: 106 | Loss: 0.013093470595777035\n",
            "Epoch: 107 | Loss: 0.012905323877930641\n",
            "Epoch: 108 | Loss: 0.012719856575131416\n",
            "Epoch: 109 | Loss: 0.012537009082734585\n",
            "Epoch: 110 | Loss: 0.012356912717223167\n",
            "Epoch: 111 | Loss: 0.012179302051663399\n",
            "Epoch: 112 | Loss: 0.012004286982119083\n",
            "Epoch: 113 | Loss: 0.011831759475171566\n",
            "Epoch: 114 | Loss: 0.011661669239401817\n",
            "Epoch: 115 | Loss: 0.011494055390357971\n",
            "Epoch: 116 | Loss: 0.011328961700201035\n",
            "Epoch: 117 | Loss: 0.011166081763803959\n",
            "Epoch: 118 | Loss: 0.011005625128746033\n",
            "Epoch: 119 | Loss: 0.010847445577383041\n",
            "Epoch: 120 | Loss: 0.010691563598811626\n",
            "Epoch: 121 | Loss: 0.01053793914616108\n",
            "Epoch: 122 | Loss: 0.010386426001787186\n",
            "Epoch: 123 | Loss: 0.010237171314656734\n",
            "Epoch: 124 | Loss: 0.010090081952512264\n",
            "Epoch: 125 | Loss: 0.009945042431354523\n",
            "Epoch: 126 | Loss: 0.009802095592021942\n",
            "Epoch: 127 | Loss: 0.00966121256351471\n",
            "Epoch: 128 | Loss: 0.009522437117993832\n",
            "Epoch: 129 | Loss: 0.009385563433170319\n",
            "Epoch: 130 | Loss: 0.009250713512301445\n",
            "Epoch: 131 | Loss: 0.009117722511291504\n",
            "Epoch: 132 | Loss: 0.008986670523881912\n",
            "Epoch: 133 | Loss: 0.008857551962137222\n",
            "Epoch: 134 | Loss: 0.008730236440896988\n",
            "Epoch: 135 | Loss: 0.008604797534644604\n",
            "Epoch: 136 | Loss: 0.008481106720864773\n",
            "Epoch: 137 | Loss: 0.008359253406524658\n",
            "Epoch: 138 | Loss: 0.008239082992076874\n",
            "Epoch: 139 | Loss: 0.008120652288198471\n",
            "Epoch: 140 | Loss: 0.008003958500921726\n",
            "Epoch: 141 | Loss: 0.007888920605182648\n",
            "Epoch: 142 | Loss: 0.007775565609335899\n",
            "Epoch: 143 | Loss: 0.0076638562604784966\n",
            "Epoch: 144 | Loss: 0.007553687319159508\n",
            "Epoch: 145 | Loss: 0.007445129100233316\n",
            "Epoch: 146 | Loss: 0.007338108494877815\n",
            "Epoch: 147 | Loss: 0.007232678588479757\n",
            "Epoch: 148 | Loss: 0.007128694094717503\n",
            "Epoch: 149 | Loss: 0.007026276551187038\n",
            "Epoch: 150 | Loss: 0.006925282068550587\n",
            "Epoch: 151 | Loss: 0.006825782358646393\n",
            "Epoch: 152 | Loss: 0.006727694533765316\n",
            "Epoch: 153 | Loss: 0.006631002761423588\n",
            "Epoch: 154 | Loss: 0.00653563579544425\n",
            "Epoch: 155 | Loss: 0.0064417351968586445\n",
            "Epoch: 156 | Loss: 0.006349171511828899\n",
            "Epoch: 157 | Loss: 0.0062579261139035225\n",
            "Epoch: 158 | Loss: 0.006167994346469641\n",
            "Epoch: 159 | Loss: 0.006079352460801601\n",
            "Epoch: 160 | Loss: 0.005991955287754536\n",
            "Epoch: 161 | Loss: 0.005905892699956894\n",
            "Epoch: 162 | Loss: 0.005820962134748697\n",
            "Epoch: 163 | Loss: 0.005737321451306343\n",
            "Epoch: 164 | Loss: 0.005654879845678806\n",
            "Epoch: 165 | Loss: 0.0055735912173986435\n",
            "Epoch: 166 | Loss: 0.005493498407304287\n",
            "Epoch: 167 | Loss: 0.005414532497525215\n",
            "Epoch: 168 | Loss: 0.005336742382496595\n",
            "Epoch: 169 | Loss: 0.00526004983112216\n",
            "Epoch: 170 | Loss: 0.005184439942240715\n",
            "Epoch: 171 | Loss: 0.005109935067594051\n",
            "Epoch: 172 | Loss: 0.005036463961005211\n",
            "Epoch: 173 | Loss: 0.004964074119925499\n",
            "Epoch: 174 | Loss: 0.00489276135340333\n",
            "Epoch: 175 | Loss: 0.004822444636374712\n",
            "Epoch: 176 | Loss: 0.004753150045871735\n",
            "Epoch: 177 | Loss: 0.004684831015765667\n",
            "Epoch: 178 | Loss: 0.004617500118911266\n",
            "Epoch: 179 | Loss: 0.004551127552986145\n",
            "Epoch: 180 | Loss: 0.004485714249312878\n",
            "Epoch: 181 | Loss: 0.0044212499633431435\n",
            "Epoch: 182 | Loss: 0.004357693716883659\n",
            "Epoch: 183 | Loss: 0.004295104183256626\n",
            "Epoch: 184 | Loss: 0.004233361221849918\n",
            "Epoch: 185 | Loss: 0.004172519315034151\n",
            "Epoch: 186 | Loss: 0.00411256542429328\n",
            "Epoch: 187 | Loss: 0.004053452983498573\n",
            "Epoch: 188 | Loss: 0.0039952159859240055\n",
            "Epoch: 189 | Loss: 0.003937806002795696\n",
            "Epoch: 190 | Loss: 0.003881184384226799\n",
            "Epoch: 191 | Loss: 0.0038254079408943653\n",
            "Epoch: 192 | Loss: 0.0037704389542341232\n",
            "Epoch: 193 | Loss: 0.0037162359803915024\n",
            "Epoch: 194 | Loss: 0.003662844654172659\n",
            "Epoch: 195 | Loss: 0.0036102025769650936\n",
            "Epoch: 196 | Loss: 0.0035583022981882095\n",
            "Epoch: 197 | Loss: 0.003507198067381978\n",
            "Epoch: 198 | Loss: 0.003456767648458481\n",
            "Epoch: 199 | Loss: 0.0034070792607963085\n",
            "Epoch: 200 | Loss: 0.003358137793838978\n",
            "Epoch: 201 | Loss: 0.003309865016490221\n",
            "Epoch: 202 | Loss: 0.0032622970174998045\n",
            "Epoch: 203 | Loss: 0.003215406322851777\n",
            "Epoch: 204 | Loss: 0.003169205505400896\n",
            "Epoch: 205 | Loss: 0.003123653819784522\n",
            "Epoch: 206 | Loss: 0.003078770823776722\n",
            "Epoch: 207 | Loss: 0.0030344948172569275\n",
            "Epoch: 208 | Loss: 0.0029909044969826937\n",
            "Epoch: 209 | Loss: 0.0029479300137609243\n",
            "Epoch: 210 | Loss: 0.0029055518098175526\n",
            "Epoch: 211 | Loss: 0.0028638034127652645\n",
            "Epoch: 212 | Loss: 0.0028226294089108706\n",
            "Epoch: 213 | Loss: 0.002782057039439678\n",
            "Epoch: 214 | Loss: 0.00274209794588387\n",
            "Epoch: 215 | Loss: 0.002702666912227869\n",
            "Epoch: 216 | Loss: 0.0026638663839548826\n",
            "Epoch: 217 | Loss: 0.0026255673728883266\n",
            "Epoch: 218 | Loss: 0.0025878408923745155\n",
            "Epoch: 219 | Loss: 0.0025506429374217987\n",
            "Epoch: 220 | Loss: 0.0025139739736914635\n",
            "Epoch: 221 | Loss: 0.002477854024618864\n",
            "Epoch: 222 | Loss: 0.002442230936139822\n",
            "Epoch: 223 | Loss: 0.002407142659649253\n",
            "Epoch: 224 | Loss: 0.002372540533542633\n",
            "Epoch: 225 | Loss: 0.0023384420201182365\n",
            "Epoch: 226 | Loss: 0.002304841298609972\n",
            "Epoch: 227 | Loss: 0.0022717085666954517\n",
            "Epoch: 228 | Loss: 0.002239065710455179\n",
            "Epoch: 229 | Loss: 0.0022068859543651342\n",
            "Epoch: 230 | Loss: 0.0021751588210463524\n",
            "Epoch: 231 | Loss: 0.002143917139619589\n",
            "Epoch: 232 | Loss: 0.0021130884997546673\n",
            "Epoch: 233 | Loss: 0.002082743216305971\n",
            "Epoch: 234 | Loss: 0.002052807016298175\n",
            "Epoch: 235 | Loss: 0.0020233020186424255\n",
            "Epoch: 236 | Loss: 0.0019942130893468857\n",
            "Epoch: 237 | Loss: 0.0019655385985970497\n",
            "Epoch: 238 | Loss: 0.0019373046234250069\n",
            "Epoch: 239 | Loss: 0.0019094705348834395\n",
            "Epoch: 240 | Loss: 0.0018820143304765224\n",
            "Epoch: 241 | Loss: 0.0018549722153693438\n",
            "Epoch: 242 | Loss: 0.0018283223034814\n",
            "Epoch: 243 | Loss: 0.0018020379357039928\n",
            "Epoch: 244 | Loss: 0.0017761474009603262\n",
            "Epoch: 245 | Loss: 0.0017506338190287352\n",
            "Epoch: 246 | Loss: 0.0017254594713449478\n",
            "Epoch: 247 | Loss: 0.0017006617272272706\n",
            "Epoch: 248 | Loss: 0.0016762338345870376\n",
            "Epoch: 249 | Loss: 0.001652129227295518\n",
            "Epoch: 250 | Loss: 0.0016283962177112699\n",
            "Epoch: 251 | Loss: 0.0016049840487539768\n",
            "Epoch: 252 | Loss: 0.001581898657605052\n",
            "Epoch: 253 | Loss: 0.0015591825358569622\n",
            "Epoch: 254 | Loss: 0.0015367742162197828\n",
            "Epoch: 255 | Loss: 0.0015146932564675808\n",
            "Epoch: 256 | Loss: 0.001492904732003808\n",
            "Epoch: 257 | Loss: 0.001471461495384574\n",
            "Epoch: 258 | Loss: 0.001450321520678699\n",
            "Epoch: 259 | Loss: 0.0014294798020273447\n",
            "Epoch: 260 | Loss: 0.001408924232237041\n",
            "Epoch: 261 | Loss: 0.0013886929955333471\n",
            "Epoch: 262 | Loss: 0.001368737081065774\n",
            "Epoch: 263 | Loss: 0.00134904938749969\n",
            "Epoch: 264 | Loss: 0.0013296803226694465\n",
            "Epoch: 265 | Loss: 0.0013105460675433278\n",
            "Epoch: 266 | Loss: 0.0012917261337861419\n",
            "Epoch: 267 | Loss: 0.0012731625465676188\n",
            "Epoch: 268 | Loss: 0.0012548548402264714\n",
            "Epoch: 269 | Loss: 0.0012368208263069391\n",
            "Epoch: 270 | Loss: 0.0012190553825348616\n",
            "Epoch: 271 | Loss: 0.0012015332467854023\n",
            "Epoch: 272 | Loss: 0.0011842622188851237\n",
            "Epoch: 273 | Loss: 0.0011672491673380136\n",
            "Epoch: 274 | Loss: 0.0011504755821079016\n",
            "Epoch: 275 | Loss: 0.0011339408811181784\n",
            "Epoch: 276 | Loss: 0.0011176425032317638\n",
            "Epoch: 277 | Loss: 0.0011015760246664286\n",
            "Epoch: 278 | Loss: 0.001085755997337401\n",
            "Epoch: 279 | Loss: 0.0010701455175876617\n",
            "Epoch: 280 | Loss: 0.0010547690326347947\n",
            "Epoch: 281 | Loss: 0.0010395955760031939\n",
            "Epoch: 282 | Loss: 0.0010246625170111656\n",
            "Epoch: 283 | Loss: 0.0010099444771185517\n",
            "Epoch: 284 | Loss: 0.0009954154957085848\n",
            "Epoch: 285 | Loss: 0.0009811247000470757\n",
            "Epoch: 286 | Loss: 0.0009670259896665812\n",
            "Epoch: 287 | Loss: 0.0009531189571134746\n",
            "Epoch: 288 | Loss: 0.000939417805057019\n",
            "Epoch: 289 | Loss: 0.0009259220096282661\n",
            "Epoch: 290 | Loss: 0.000912627438083291\n",
            "Epoch: 291 | Loss: 0.0008994988165795803\n",
            "Epoch: 292 | Loss: 0.0008865674026310444\n",
            "Epoch: 293 | Loss: 0.0008738328469917178\n",
            "Epoch: 294 | Loss: 0.0008612739620730281\n",
            "Epoch: 295 | Loss: 0.0008488873136229813\n",
            "Epoch: 296 | Loss: 0.0008366929832845926\n",
            "Epoch: 297 | Loss: 0.0008246669895015657\n",
            "Epoch: 298 | Loss: 0.000812822487205267\n",
            "Epoch: 299 | Loss: 0.000801132875494659\n",
            "Epoch: 300 | Loss: 0.0007896243478171527\n",
            "Epoch: 301 | Loss: 0.000778265530243516\n",
            "Epoch: 302 | Loss: 0.0007670919876545668\n",
            "Epoch: 303 | Loss: 0.0007560585509054363\n",
            "Epoch: 304 | Loss: 0.0007452033460140228\n",
            "Epoch: 305 | Loss: 0.0007344740442931652\n",
            "Epoch: 306 | Loss: 0.0007239225087687373\n",
            "Epoch: 307 | Loss: 0.0007135234773159027\n",
            "Epoch: 308 | Loss: 0.0007032660068944097\n",
            "Epoch: 309 | Loss: 0.0006931715179234743\n",
            "Epoch: 310 | Loss: 0.0006832093931734562\n",
            "Epoch: 311 | Loss: 0.0006733781192451715\n",
            "Epoch: 312 | Loss: 0.0006637049955315888\n",
            "Epoch: 313 | Loss: 0.0006541584152728319\n",
            "Epoch: 314 | Loss: 0.0006447650375775993\n",
            "Epoch: 315 | Loss: 0.0006354983197525144\n",
            "Epoch: 316 | Loss: 0.0006263742689043283\n",
            "Epoch: 317 | Loss: 0.0006173725705593824\n",
            "Epoch: 318 | Loss: 0.0006084962515160441\n",
            "Epoch: 319 | Loss: 0.0005997512489557266\n",
            "Epoch: 320 | Loss: 0.0005911316256970167\n",
            "Epoch: 321 | Loss: 0.0005826347041875124\n",
            "Epoch: 322 | Loss: 0.0005742635112255812\n",
            "Epoch: 323 | Loss: 0.0005660178139805794\n",
            "Epoch: 324 | Loss: 0.0005578812560997903\n",
            "Epoch: 325 | Loss: 0.0005498540122061968\n",
            "Epoch: 326 | Loss: 0.0005419566878117621\n",
            "Epoch: 327 | Loss: 0.0005341568030416965\n",
            "Epoch: 328 | Loss: 0.000526489457115531\n",
            "Epoch: 329 | Loss: 0.0005189318908378482\n",
            "Epoch: 330 | Loss: 0.000511456630192697\n",
            "Epoch: 331 | Loss: 0.0005041229305788875\n",
            "Epoch: 332 | Loss: 0.0004968721186742187\n",
            "Epoch: 333 | Loss: 0.0004897394101135433\n",
            "Epoch: 334 | Loss: 0.00048268891987390816\n",
            "Epoch: 335 | Loss: 0.0004757643910124898\n",
            "Epoch: 336 | Loss: 0.00046891631791368127\n",
            "Epoch: 337 | Loss: 0.00046218791976571083\n",
            "Epoch: 338 | Loss: 0.00045554162352345884\n",
            "Epoch: 339 | Loss: 0.0004489903221838176\n",
            "Epoch: 340 | Loss: 0.000442530435975641\n",
            "Epoch: 341 | Loss: 0.0004361744795460254\n",
            "Epoch: 342 | Loss: 0.00042990909423679113\n",
            "Epoch: 343 | Loss: 0.0004237308748997748\n",
            "Epoch: 344 | Loss: 0.00041763545596040785\n",
            "Epoch: 345 | Loss: 0.0004116302006877959\n",
            "Epoch: 346 | Loss: 0.00040571644785813987\n",
            "Epoch: 347 | Loss: 0.0003998850006610155\n",
            "Epoch: 348 | Loss: 0.00039414424099959433\n",
            "Epoch: 349 | Loss: 0.0003884793841280043\n",
            "Epoch: 350 | Loss: 0.00038290105294436216\n",
            "Epoch: 351 | Loss: 0.00037739466642960906\n",
            "Epoch: 352 | Loss: 0.00037196968332864344\n",
            "Epoch: 353 | Loss: 0.0003666240372695029\n",
            "Epoch: 354 | Loss: 0.0003613602020777762\n",
            "Epoch: 355 | Loss: 0.00035616953391581774\n",
            "Epoch: 356 | Loss: 0.00035103599657304585\n",
            "Epoch: 357 | Loss: 0.00034600141225382686\n",
            "Epoch: 358 | Loss: 0.00034103216603398323\n",
            "Epoch: 359 | Loss: 0.0003361286944709718\n",
            "Epoch: 360 | Loss: 0.00033129676012322307\n",
            "Epoch: 361 | Loss: 0.00032652815571054816\n",
            "Epoch: 362 | Loss: 0.00032184208976104856\n",
            "Epoch: 363 | Loss: 0.00031721265986561775\n",
            "Epoch: 364 | Loss: 0.00031265587313100696\n",
            "Epoch: 365 | Loss: 0.00030816264916211367\n",
            "Epoch: 366 | Loss: 0.0003037313581444323\n",
            "Epoch: 367 | Loss: 0.00029936330975033343\n",
            "Epoch: 368 | Loss: 0.00029507087310776114\n",
            "Epoch: 369 | Loss: 0.0002908263122662902\n",
            "Epoch: 370 | Loss: 0.0002866479044314474\n",
            "Epoch: 371 | Loss: 0.00028252805350348353\n",
            "Epoch: 372 | Loss: 0.00027846713783219457\n",
            "Epoch: 373 | Loss: 0.00027446457534097135\n",
            "Epoch: 374 | Loss: 0.00027051963843405247\n",
            "Epoch: 375 | Loss: 0.0002666383807081729\n",
            "Epoch: 376 | Loss: 0.00026279271696694195\n",
            "Epoch: 377 | Loss: 0.00025902455672621727\n",
            "Epoch: 378 | Loss: 0.0002553050289861858\n",
            "Epoch: 379 | Loss: 0.00025162988458760083\n",
            "Epoch: 380 | Loss: 0.0002480180119164288\n",
            "Epoch: 381 | Loss: 0.0002444441197440028\n",
            "Epoch: 382 | Loss: 0.00024093851970974356\n",
            "Epoch: 383 | Loss: 0.00023747081286273897\n",
            "Epoch: 384 | Loss: 0.000234068269492127\n",
            "Epoch: 385 | Loss: 0.00023069733288139105\n",
            "Epoch: 386 | Loss: 0.00022738330881111324\n",
            "Epoch: 387 | Loss: 0.00022411238751374185\n",
            "Epoch: 388 | Loss: 0.0002208988880738616\n",
            "Epoch: 389 | Loss: 0.00021772058971691877\n",
            "Epoch: 390 | Loss: 0.00021458830451592803\n",
            "Epoch: 391 | Loss: 0.00021150073735043406\n",
            "Epoch: 392 | Loss: 0.0002084665757138282\n",
            "Epoch: 393 | Loss: 0.00020546602900139987\n",
            "Epoch: 394 | Loss: 0.00020251782552804798\n",
            "Epoch: 395 | Loss: 0.00019960899953730404\n",
            "Epoch: 396 | Loss: 0.00019673429778777063\n",
            "Epoch: 397 | Loss: 0.0001939168432727456\n",
            "Epoch: 398 | Loss: 0.00019112696463707834\n",
            "Epoch: 399 | Loss: 0.00018837406241800636\n",
            "Epoch: 400 | Loss: 0.0001856704184319824\n",
            "Epoch: 401 | Loss: 0.00018299736257176846\n",
            "Epoch: 402 | Loss: 0.00018037490372080356\n",
            "Epoch: 403 | Loss: 0.00017777603352442384\n",
            "Epoch: 404 | Loss: 0.00017522448615636677\n",
            "Epoch: 405 | Loss: 0.00017270819807890803\n",
            "Epoch: 406 | Loss: 0.0001702283334452659\n",
            "Epoch: 407 | Loss: 0.00016777540440671146\n",
            "Epoch: 408 | Loss: 0.00016536348266527057\n",
            "Epoch: 409 | Loss: 0.00016299351409543306\n",
            "Epoch: 410 | Loss: 0.0001606428559171036\n",
            "Epoch: 411 | Loss: 0.00015833853103686124\n",
            "Epoch: 412 | Loss: 0.0001560617529321462\n",
            "Epoch: 413 | Loss: 0.00015382166020572186\n",
            "Epoch: 414 | Loss: 0.00015160994371399283\n",
            "Epoch: 415 | Loss: 0.00014943057612981647\n",
            "Epoch: 416 | Loss: 0.00014728674432262778\n",
            "Epoch: 417 | Loss: 0.0001451703137718141\n",
            "Epoch: 418 | Loss: 0.00014308241952676326\n",
            "Epoch: 419 | Loss: 0.00014102138811722398\n",
            "Epoch: 420 | Loss: 0.0001389958633808419\n",
            "Epoch: 421 | Loss: 0.0001369945239275694\n",
            "Epoch: 422 | Loss: 0.0001350293168798089\n",
            "Epoch: 423 | Loss: 0.00013309306814335287\n",
            "Epoch: 424 | Loss: 0.00013118013157509267\n",
            "Epoch: 425 | Loss: 0.0001292902888962999\n",
            "Epoch: 426 | Loss: 0.000127435167087242\n",
            "Epoch: 427 | Loss: 0.00012560124741867185\n",
            "Epoch: 428 | Loss: 0.0001238013501279056\n",
            "Epoch: 429 | Loss: 0.00012201508798170835\n",
            "Epoch: 430 | Loss: 0.00012025964679196477\n",
            "Epoch: 431 | Loss: 0.00011853718024212867\n",
            "Epoch: 432 | Loss: 0.00011683090997394174\n",
            "Epoch: 433 | Loss: 0.00011515572259668261\n",
            "Epoch: 434 | Loss: 0.00011349203123245388\n",
            "Epoch: 435 | Loss: 0.00011186317715328187\n",
            "Epoch: 436 | Loss: 0.00011026137508451939\n",
            "Epoch: 437 | Loss: 0.0001086747579392977\n",
            "Epoch: 438 | Loss: 0.00010710866627050564\n",
            "Epoch: 439 | Loss: 0.00010557125642662868\n",
            "Epoch: 440 | Loss: 0.00010405505599919707\n",
            "Epoch: 441 | Loss: 0.00010256277164444327\n",
            "Epoch: 442 | Loss: 0.00010108535934705287\n",
            "Epoch: 443 | Loss: 9.963143384084105e-05\n",
            "Epoch: 444 | Loss: 9.820014383876696e-05\n",
            "Epoch: 445 | Loss: 9.678950300440192e-05\n",
            "Epoch: 446 | Loss: 9.53998533077538e-05\n",
            "Epoch: 447 | Loss: 9.402588329976425e-05\n",
            "Epoch: 448 | Loss: 9.267364657716826e-05\n",
            "Epoch: 449 | Loss: 9.134397987509146e-05\n",
            "Epoch: 450 | Loss: 9.003329614643008e-05\n",
            "Epoch: 451 | Loss: 8.873811748344451e-05\n",
            "Epoch: 452 | Loss: 8.746157254790887e-05\n",
            "Epoch: 453 | Loss: 8.620235894341022e-05\n",
            "Epoch: 454 | Loss: 8.496567897964269e-05\n",
            "Epoch: 455 | Loss: 8.374378376174718e-05\n",
            "Epoch: 456 | Loss: 8.254395652329549e-05\n",
            "Epoch: 457 | Loss: 8.135593088809401e-05\n",
            "Epoch: 458 | Loss: 8.018848893698305e-05\n",
            "Epoch: 459 | Loss: 7.903309597168118e-05\n",
            "Epoch: 460 | Loss: 7.78968824306503e-05\n",
            "Epoch: 461 | Loss: 7.67780511523597e-05\n",
            "Epoch: 462 | Loss: 7.56733788875863e-05\n",
            "Epoch: 463 | Loss: 7.458877371391281e-05\n",
            "Epoch: 464 | Loss: 7.351698150159791e-05\n",
            "Epoch: 465 | Loss: 7.245838787639514e-05\n",
            "Epoch: 466 | Loss: 7.141876994865015e-05\n",
            "Epoch: 467 | Loss: 7.039106276351959e-05\n",
            "Epoch: 468 | Loss: 6.938193109817803e-05\n",
            "Epoch: 469 | Loss: 6.838297849753872e-05\n",
            "Epoch: 470 | Loss: 6.740080425515771e-05\n",
            "Epoch: 471 | Loss: 6.643144297413528e-05\n",
            "Epoch: 472 | Loss: 6.547473458340392e-05\n",
            "Epoch: 473 | Loss: 6.453291280195117e-05\n",
            "Epoch: 474 | Loss: 6.360671250149608e-05\n",
            "Epoch: 475 | Loss: 6.269320874707773e-05\n",
            "Epoch: 476 | Loss: 6.179499177960679e-05\n",
            "Epoch: 477 | Loss: 6.090642273193225e-05\n",
            "Epoch: 478 | Loss: 6.002744703437202e-05\n",
            "Epoch: 479 | Loss: 5.9166042774450034e-05\n",
            "Epoch: 480 | Loss: 5.8312638429924846e-05\n",
            "Epoch: 481 | Loss: 5.747645627707243e-05\n",
            "Epoch: 482 | Loss: 5.6652868806850165e-05\n",
            "Epoch: 483 | Loss: 5.5837845138739794e-05\n",
            "Epoch: 484 | Loss: 5.503863576450385e-05\n",
            "Epoch: 485 | Loss: 5.424605114967562e-05\n",
            "Epoch: 486 | Loss: 5.3467287216335535e-05\n",
            "Epoch: 487 | Loss: 5.269500979920849e-05\n",
            "Epoch: 488 | Loss: 5.193881952436641e-05\n",
            "Epoch: 489 | Loss: 5.1191836973885074e-05\n",
            "Epoch: 490 | Loss: 5.0457692850613967e-05\n",
            "Epoch: 491 | Loss: 4.972967872163281e-05\n",
            "Epoch: 492 | Loss: 4.9014692194759846e-05\n",
            "Epoch: 493 | Loss: 4.8312151193385944e-05\n",
            "Epoch: 494 | Loss: 4.761909804074094e-05\n",
            "Epoch: 495 | Loss: 4.693503069574945e-05\n",
            "Epoch: 496 | Loss: 4.6261848183348775e-05\n",
            "Epoch: 497 | Loss: 4.5591179514303803e-05\n",
            "Epoch: 498 | Loss: 4.493904270930216e-05\n",
            "Epoch: 499 | Loss: 4.429430919117294e-05\n",
            "Prediction (after training) 4 7.992349624633789\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0njTWoKsXjfl"
      },
      "source": [
        "## logistic regression model\n",
        "\n",
        "- 종속변수가 실수가 아니라, 상태인 경우(부류)\n",
        "- softmax와 cross entropy를 사용함!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gXoi0QjPSJD",
        "outputId": "3da9169c-6fef-472d-8468-a0519cb6aee3"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import tensor\n",
        "from torch import sigmoid\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "x_data = tensor([[1.0], [2.0], [3.0], [4.0]])\n",
        "y_data = tensor([[0.], [0.], [1.], [1.]]) #binary prediction 문제로 변환\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__() #생성자 호출\n",
        "        self.linear = nn.Linear(1,1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        y_pred = sigmoid(self.linear(x)) #sigmoid 단순히 매핑시키면 되서, forward에서 정의함\n",
        "        return y_pred\n",
        "\n",
        "#모델\n",
        "model = Model()\n",
        "\n",
        "criterion = nn.BCELoss(reduction='mean') # binary cross entropy loss\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "#Training Loop\n",
        "for epoch in range(1000):\n",
        "    #forward\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    #compute loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch {epoch+1}/1000 | Loss : {loss.item():.4f}')\n",
        "\n",
        "    #backward and update\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "#after training\n",
        "print(f'\\nLet\\'s predict the hours need to score above 50%\\n{\"=\"*50}')\n",
        "hour_var = model(tensor([[1.0]]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() > 0.5}')\n",
        "hour_var = model(tensor([[7.0]]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() > 0.5}')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000 | Loss : 0.7805\n",
            "Epoch 2/1000 | Loss : 0.7764\n",
            "Epoch 3/1000 | Loss : 0.7724\n",
            "Epoch 4/1000 | Loss : 0.7684\n",
            "Epoch 5/1000 | Loss : 0.7645\n",
            "Epoch 6/1000 | Loss : 0.7606\n",
            "Epoch 7/1000 | Loss : 0.7568\n",
            "Epoch 8/1000 | Loss : 0.7530\n",
            "Epoch 9/1000 | Loss : 0.7492\n",
            "Epoch 10/1000 | Loss : 0.7454\n",
            "Epoch 11/1000 | Loss : 0.7418\n",
            "Epoch 12/1000 | Loss : 0.7381\n",
            "Epoch 13/1000 | Loss : 0.7345\n",
            "Epoch 14/1000 | Loss : 0.7309\n",
            "Epoch 15/1000 | Loss : 0.7274\n",
            "Epoch 16/1000 | Loss : 0.7239\n",
            "Epoch 17/1000 | Loss : 0.7204\n",
            "Epoch 18/1000 | Loss : 0.7170\n",
            "Epoch 19/1000 | Loss : 0.7137\n",
            "Epoch 20/1000 | Loss : 0.7103\n",
            "Epoch 21/1000 | Loss : 0.7070\n",
            "Epoch 22/1000 | Loss : 0.7038\n",
            "Epoch 23/1000 | Loss : 0.7006\n",
            "Epoch 24/1000 | Loss : 0.6974\n",
            "Epoch 25/1000 | Loss : 0.6943\n",
            "Epoch 26/1000 | Loss : 0.6912\n",
            "Epoch 27/1000 | Loss : 0.6881\n",
            "Epoch 28/1000 | Loss : 0.6851\n",
            "Epoch 29/1000 | Loss : 0.6821\n",
            "Epoch 30/1000 | Loss : 0.6792\n",
            "Epoch 31/1000 | Loss : 0.6763\n",
            "Epoch 32/1000 | Loss : 0.6734\n",
            "Epoch 33/1000 | Loss : 0.6706\n",
            "Epoch 34/1000 | Loss : 0.6678\n",
            "Epoch 35/1000 | Loss : 0.6651\n",
            "Epoch 36/1000 | Loss : 0.6624\n",
            "Epoch 37/1000 | Loss : 0.6597\n",
            "Epoch 38/1000 | Loss : 0.6571\n",
            "Epoch 39/1000 | Loss : 0.6545\n",
            "Epoch 40/1000 | Loss : 0.6520\n",
            "Epoch 41/1000 | Loss : 0.6494\n",
            "Epoch 42/1000 | Loss : 0.6470\n",
            "Epoch 43/1000 | Loss : 0.6445\n",
            "Epoch 44/1000 | Loss : 0.6421\n",
            "Epoch 45/1000 | Loss : 0.6398\n",
            "Epoch 46/1000 | Loss : 0.6374\n",
            "Epoch 47/1000 | Loss : 0.6351\n",
            "Epoch 48/1000 | Loss : 0.6329\n",
            "Epoch 49/1000 | Loss : 0.6306\n",
            "Epoch 50/1000 | Loss : 0.6285\n",
            "Epoch 51/1000 | Loss : 0.6263\n",
            "Epoch 52/1000 | Loss : 0.6242\n",
            "Epoch 53/1000 | Loss : 0.6221\n",
            "Epoch 54/1000 | Loss : 0.6201\n",
            "Epoch 55/1000 | Loss : 0.6180\n",
            "Epoch 56/1000 | Loss : 0.6161\n",
            "Epoch 57/1000 | Loss : 0.6141\n",
            "Epoch 58/1000 | Loss : 0.6122\n",
            "Epoch 59/1000 | Loss : 0.6103\n",
            "Epoch 60/1000 | Loss : 0.6084\n",
            "Epoch 61/1000 | Loss : 0.6066\n",
            "Epoch 62/1000 | Loss : 0.6048\n",
            "Epoch 63/1000 | Loss : 0.6031\n",
            "Epoch 64/1000 | Loss : 0.6013\n",
            "Epoch 65/1000 | Loss : 0.5996\n",
            "Epoch 66/1000 | Loss : 0.5980\n",
            "Epoch 67/1000 | Loss : 0.5963\n",
            "Epoch 68/1000 | Loss : 0.5947\n",
            "Epoch 69/1000 | Loss : 0.5932\n",
            "Epoch 70/1000 | Loss : 0.5916\n",
            "Epoch 71/1000 | Loss : 0.5901\n",
            "Epoch 72/1000 | Loss : 0.5886\n",
            "Epoch 73/1000 | Loss : 0.5871\n",
            "Epoch 74/1000 | Loss : 0.5857\n",
            "Epoch 75/1000 | Loss : 0.5843\n",
            "Epoch 76/1000 | Loss : 0.5829\n",
            "Epoch 77/1000 | Loss : 0.5815\n",
            "Epoch 78/1000 | Loss : 0.5802\n",
            "Epoch 79/1000 | Loss : 0.5788\n",
            "Epoch 80/1000 | Loss : 0.5776\n",
            "Epoch 81/1000 | Loss : 0.5763\n",
            "Epoch 82/1000 | Loss : 0.5751\n",
            "Epoch 83/1000 | Loss : 0.5738\n",
            "Epoch 84/1000 | Loss : 0.5726\n",
            "Epoch 85/1000 | Loss : 0.5715\n",
            "Epoch 86/1000 | Loss : 0.5703\n",
            "Epoch 87/1000 | Loss : 0.5692\n",
            "Epoch 88/1000 | Loss : 0.5681\n",
            "Epoch 89/1000 | Loss : 0.5670\n",
            "Epoch 90/1000 | Loss : 0.5659\n",
            "Epoch 91/1000 | Loss : 0.5649\n",
            "Epoch 92/1000 | Loss : 0.5639\n",
            "Epoch 93/1000 | Loss : 0.5629\n",
            "Epoch 94/1000 | Loss : 0.5619\n",
            "Epoch 95/1000 | Loss : 0.5609\n",
            "Epoch 96/1000 | Loss : 0.5600\n",
            "Epoch 97/1000 | Loss : 0.5591\n",
            "Epoch 98/1000 | Loss : 0.5581\n",
            "Epoch 99/1000 | Loss : 0.5573\n",
            "Epoch 100/1000 | Loss : 0.5564\n",
            "Epoch 101/1000 | Loss : 0.5555\n",
            "Epoch 102/1000 | Loss : 0.5547\n",
            "Epoch 103/1000 | Loss : 0.5539\n",
            "Epoch 104/1000 | Loss : 0.5531\n",
            "Epoch 105/1000 | Loss : 0.5523\n",
            "Epoch 106/1000 | Loss : 0.5515\n",
            "Epoch 107/1000 | Loss : 0.5507\n",
            "Epoch 108/1000 | Loss : 0.5500\n",
            "Epoch 109/1000 | Loss : 0.5492\n",
            "Epoch 110/1000 | Loss : 0.5485\n",
            "Epoch 111/1000 | Loss : 0.5478\n",
            "Epoch 112/1000 | Loss : 0.5471\n",
            "Epoch 113/1000 | Loss : 0.5464\n",
            "Epoch 114/1000 | Loss : 0.5458\n",
            "Epoch 115/1000 | Loss : 0.5451\n",
            "Epoch 116/1000 | Loss : 0.5445\n",
            "Epoch 117/1000 | Loss : 0.5439\n",
            "Epoch 118/1000 | Loss : 0.5432\n",
            "Epoch 119/1000 | Loss : 0.5426\n",
            "Epoch 120/1000 | Loss : 0.5420\n",
            "Epoch 121/1000 | Loss : 0.5415\n",
            "Epoch 122/1000 | Loss : 0.5409\n",
            "Epoch 123/1000 | Loss : 0.5403\n",
            "Epoch 124/1000 | Loss : 0.5398\n",
            "Epoch 125/1000 | Loss : 0.5392\n",
            "Epoch 126/1000 | Loss : 0.5387\n",
            "Epoch 127/1000 | Loss : 0.5382\n",
            "Epoch 128/1000 | Loss : 0.5377\n",
            "Epoch 129/1000 | Loss : 0.5372\n",
            "Epoch 130/1000 | Loss : 0.5367\n",
            "Epoch 131/1000 | Loss : 0.5362\n",
            "Epoch 132/1000 | Loss : 0.5357\n",
            "Epoch 133/1000 | Loss : 0.5352\n",
            "Epoch 134/1000 | Loss : 0.5348\n",
            "Epoch 135/1000 | Loss : 0.5343\n",
            "Epoch 136/1000 | Loss : 0.5339\n",
            "Epoch 137/1000 | Loss : 0.5334\n",
            "Epoch 138/1000 | Loss : 0.5330\n",
            "Epoch 139/1000 | Loss : 0.5326\n",
            "Epoch 140/1000 | Loss : 0.5322\n",
            "Epoch 141/1000 | Loss : 0.5317\n",
            "Epoch 142/1000 | Loss : 0.5313\n",
            "Epoch 143/1000 | Loss : 0.5309\n",
            "Epoch 144/1000 | Loss : 0.5306\n",
            "Epoch 145/1000 | Loss : 0.5302\n",
            "Epoch 146/1000 | Loss : 0.5298\n",
            "Epoch 147/1000 | Loss : 0.5294\n",
            "Epoch 148/1000 | Loss : 0.5290\n",
            "Epoch 149/1000 | Loss : 0.5287\n",
            "Epoch 150/1000 | Loss : 0.5283\n",
            "Epoch 151/1000 | Loss : 0.5280\n",
            "Epoch 152/1000 | Loss : 0.5276\n",
            "Epoch 153/1000 | Loss : 0.5273\n",
            "Epoch 154/1000 | Loss : 0.5269\n",
            "Epoch 155/1000 | Loss : 0.5266\n",
            "Epoch 156/1000 | Loss : 0.5263\n",
            "Epoch 157/1000 | Loss : 0.5260\n",
            "Epoch 158/1000 | Loss : 0.5256\n",
            "Epoch 159/1000 | Loss : 0.5253\n",
            "Epoch 160/1000 | Loss : 0.5250\n",
            "Epoch 161/1000 | Loss : 0.5247\n",
            "Epoch 162/1000 | Loss : 0.5244\n",
            "Epoch 163/1000 | Loss : 0.5241\n",
            "Epoch 164/1000 | Loss : 0.5238\n",
            "Epoch 165/1000 | Loss : 0.5235\n",
            "Epoch 166/1000 | Loss : 0.5232\n",
            "Epoch 167/1000 | Loss : 0.5229\n",
            "Epoch 168/1000 | Loss : 0.5227\n",
            "Epoch 169/1000 | Loss : 0.5224\n",
            "Epoch 170/1000 | Loss : 0.5221\n",
            "Epoch 171/1000 | Loss : 0.5218\n",
            "Epoch 172/1000 | Loss : 0.5216\n",
            "Epoch 173/1000 | Loss : 0.5213\n",
            "Epoch 174/1000 | Loss : 0.5210\n",
            "Epoch 175/1000 | Loss : 0.5208\n",
            "Epoch 176/1000 | Loss : 0.5205\n",
            "Epoch 177/1000 | Loss : 0.5202\n",
            "Epoch 178/1000 | Loss : 0.5200\n",
            "Epoch 179/1000 | Loss : 0.5197\n",
            "Epoch 180/1000 | Loss : 0.5195\n",
            "Epoch 181/1000 | Loss : 0.5192\n",
            "Epoch 182/1000 | Loss : 0.5190\n",
            "Epoch 183/1000 | Loss : 0.5188\n",
            "Epoch 184/1000 | Loss : 0.5185\n",
            "Epoch 185/1000 | Loss : 0.5183\n",
            "Epoch 186/1000 | Loss : 0.5180\n",
            "Epoch 187/1000 | Loss : 0.5178\n",
            "Epoch 188/1000 | Loss : 0.5176\n",
            "Epoch 189/1000 | Loss : 0.5173\n",
            "Epoch 190/1000 | Loss : 0.5171\n",
            "Epoch 191/1000 | Loss : 0.5169\n",
            "Epoch 192/1000 | Loss : 0.5166\n",
            "Epoch 193/1000 | Loss : 0.5164\n",
            "Epoch 194/1000 | Loss : 0.5162\n",
            "Epoch 195/1000 | Loss : 0.5160\n",
            "Epoch 196/1000 | Loss : 0.5158\n",
            "Epoch 197/1000 | Loss : 0.5155\n",
            "Epoch 198/1000 | Loss : 0.5153\n",
            "Epoch 199/1000 | Loss : 0.5151\n",
            "Epoch 200/1000 | Loss : 0.5149\n",
            "Epoch 201/1000 | Loss : 0.5147\n",
            "Epoch 202/1000 | Loss : 0.5145\n",
            "Epoch 203/1000 | Loss : 0.5143\n",
            "Epoch 204/1000 | Loss : 0.5140\n",
            "Epoch 205/1000 | Loss : 0.5138\n",
            "Epoch 206/1000 | Loss : 0.5136\n",
            "Epoch 207/1000 | Loss : 0.5134\n",
            "Epoch 208/1000 | Loss : 0.5132\n",
            "Epoch 209/1000 | Loss : 0.5130\n",
            "Epoch 210/1000 | Loss : 0.5128\n",
            "Epoch 211/1000 | Loss : 0.5126\n",
            "Epoch 212/1000 | Loss : 0.5124\n",
            "Epoch 213/1000 | Loss : 0.5122\n",
            "Epoch 214/1000 | Loss : 0.5120\n",
            "Epoch 215/1000 | Loss : 0.5118\n",
            "Epoch 216/1000 | Loss : 0.5116\n",
            "Epoch 217/1000 | Loss : 0.5114\n",
            "Epoch 218/1000 | Loss : 0.5112\n",
            "Epoch 219/1000 | Loss : 0.5110\n",
            "Epoch 220/1000 | Loss : 0.5108\n",
            "Epoch 221/1000 | Loss : 0.5106\n",
            "Epoch 222/1000 | Loss : 0.5104\n",
            "Epoch 223/1000 | Loss : 0.5102\n",
            "Epoch 224/1000 | Loss : 0.5101\n",
            "Epoch 225/1000 | Loss : 0.5099\n",
            "Epoch 226/1000 | Loss : 0.5097\n",
            "Epoch 227/1000 | Loss : 0.5095\n",
            "Epoch 228/1000 | Loss : 0.5093\n",
            "Epoch 229/1000 | Loss : 0.5091\n",
            "Epoch 230/1000 | Loss : 0.5089\n",
            "Epoch 231/1000 | Loss : 0.5087\n",
            "Epoch 232/1000 | Loss : 0.5085\n",
            "Epoch 233/1000 | Loss : 0.5084\n",
            "Epoch 234/1000 | Loss : 0.5082\n",
            "Epoch 235/1000 | Loss : 0.5080\n",
            "Epoch 236/1000 | Loss : 0.5078\n",
            "Epoch 237/1000 | Loss : 0.5076\n",
            "Epoch 238/1000 | Loss : 0.5074\n",
            "Epoch 239/1000 | Loss : 0.5072\n",
            "Epoch 240/1000 | Loss : 0.5071\n",
            "Epoch 241/1000 | Loss : 0.5069\n",
            "Epoch 242/1000 | Loss : 0.5067\n",
            "Epoch 243/1000 | Loss : 0.5065\n",
            "Epoch 244/1000 | Loss : 0.5063\n",
            "Epoch 245/1000 | Loss : 0.5062\n",
            "Epoch 246/1000 | Loss : 0.5060\n",
            "Epoch 247/1000 | Loss : 0.5058\n",
            "Epoch 248/1000 | Loss : 0.5056\n",
            "Epoch 249/1000 | Loss : 0.5054\n",
            "Epoch 250/1000 | Loss : 0.5053\n",
            "Epoch 251/1000 | Loss : 0.5051\n",
            "Epoch 252/1000 | Loss : 0.5049\n",
            "Epoch 253/1000 | Loss : 0.5047\n",
            "Epoch 254/1000 | Loss : 0.5045\n",
            "Epoch 255/1000 | Loss : 0.5044\n",
            "Epoch 256/1000 | Loss : 0.5042\n",
            "Epoch 257/1000 | Loss : 0.5040\n",
            "Epoch 258/1000 | Loss : 0.5038\n",
            "Epoch 259/1000 | Loss : 0.5037\n",
            "Epoch 260/1000 | Loss : 0.5035\n",
            "Epoch 261/1000 | Loss : 0.5033\n",
            "Epoch 262/1000 | Loss : 0.5031\n",
            "Epoch 263/1000 | Loss : 0.5030\n",
            "Epoch 264/1000 | Loss : 0.5028\n",
            "Epoch 265/1000 | Loss : 0.5026\n",
            "Epoch 266/1000 | Loss : 0.5024\n",
            "Epoch 267/1000 | Loss : 0.5023\n",
            "Epoch 268/1000 | Loss : 0.5021\n",
            "Epoch 269/1000 | Loss : 0.5019\n",
            "Epoch 270/1000 | Loss : 0.5018\n",
            "Epoch 271/1000 | Loss : 0.5016\n",
            "Epoch 272/1000 | Loss : 0.5014\n",
            "Epoch 273/1000 | Loss : 0.5012\n",
            "Epoch 274/1000 | Loss : 0.5011\n",
            "Epoch 275/1000 | Loss : 0.5009\n",
            "Epoch 276/1000 | Loss : 0.5007\n",
            "Epoch 277/1000 | Loss : 0.5005\n",
            "Epoch 278/1000 | Loss : 0.5004\n",
            "Epoch 279/1000 | Loss : 0.5002\n",
            "Epoch 280/1000 | Loss : 0.5000\n",
            "Epoch 281/1000 | Loss : 0.4999\n",
            "Epoch 282/1000 | Loss : 0.4997\n",
            "Epoch 283/1000 | Loss : 0.4995\n",
            "Epoch 284/1000 | Loss : 0.4994\n",
            "Epoch 285/1000 | Loss : 0.4992\n",
            "Epoch 286/1000 | Loss : 0.4990\n",
            "Epoch 287/1000 | Loss : 0.4988\n",
            "Epoch 288/1000 | Loss : 0.4987\n",
            "Epoch 289/1000 | Loss : 0.4985\n",
            "Epoch 290/1000 | Loss : 0.4983\n",
            "Epoch 291/1000 | Loss : 0.4982\n",
            "Epoch 292/1000 | Loss : 0.4980\n",
            "Epoch 293/1000 | Loss : 0.4978\n",
            "Epoch 294/1000 | Loss : 0.4977\n",
            "Epoch 295/1000 | Loss : 0.4975\n",
            "Epoch 296/1000 | Loss : 0.4973\n",
            "Epoch 297/1000 | Loss : 0.4972\n",
            "Epoch 298/1000 | Loss : 0.4970\n",
            "Epoch 299/1000 | Loss : 0.4968\n",
            "Epoch 300/1000 | Loss : 0.4967\n",
            "Epoch 301/1000 | Loss : 0.4965\n",
            "Epoch 302/1000 | Loss : 0.4963\n",
            "Epoch 303/1000 | Loss : 0.4962\n",
            "Epoch 304/1000 | Loss : 0.4960\n",
            "Epoch 305/1000 | Loss : 0.4958\n",
            "Epoch 306/1000 | Loss : 0.4957\n",
            "Epoch 307/1000 | Loss : 0.4955\n",
            "Epoch 308/1000 | Loss : 0.4953\n",
            "Epoch 309/1000 | Loss : 0.4952\n",
            "Epoch 310/1000 | Loss : 0.4950\n",
            "Epoch 311/1000 | Loss : 0.4948\n",
            "Epoch 312/1000 | Loss : 0.4947\n",
            "Epoch 313/1000 | Loss : 0.4945\n",
            "Epoch 314/1000 | Loss : 0.4943\n",
            "Epoch 315/1000 | Loss : 0.4942\n",
            "Epoch 316/1000 | Loss : 0.4940\n",
            "Epoch 317/1000 | Loss : 0.4939\n",
            "Epoch 318/1000 | Loss : 0.4937\n",
            "Epoch 319/1000 | Loss : 0.4935\n",
            "Epoch 320/1000 | Loss : 0.4934\n",
            "Epoch 321/1000 | Loss : 0.4932\n",
            "Epoch 322/1000 | Loss : 0.4930\n",
            "Epoch 323/1000 | Loss : 0.4929\n",
            "Epoch 324/1000 | Loss : 0.4927\n",
            "Epoch 325/1000 | Loss : 0.4925\n",
            "Epoch 326/1000 | Loss : 0.4924\n",
            "Epoch 327/1000 | Loss : 0.4922\n",
            "Epoch 328/1000 | Loss : 0.4921\n",
            "Epoch 329/1000 | Loss : 0.4919\n",
            "Epoch 330/1000 | Loss : 0.4917\n",
            "Epoch 331/1000 | Loss : 0.4916\n",
            "Epoch 332/1000 | Loss : 0.4914\n",
            "Epoch 333/1000 | Loss : 0.4912\n",
            "Epoch 334/1000 | Loss : 0.4911\n",
            "Epoch 335/1000 | Loss : 0.4909\n",
            "Epoch 336/1000 | Loss : 0.4908\n",
            "Epoch 337/1000 | Loss : 0.4906\n",
            "Epoch 338/1000 | Loss : 0.4904\n",
            "Epoch 339/1000 | Loss : 0.4903\n",
            "Epoch 340/1000 | Loss : 0.4901\n",
            "Epoch 341/1000 | Loss : 0.4899\n",
            "Epoch 342/1000 | Loss : 0.4898\n",
            "Epoch 343/1000 | Loss : 0.4896\n",
            "Epoch 344/1000 | Loss : 0.4895\n",
            "Epoch 345/1000 | Loss : 0.4893\n",
            "Epoch 346/1000 | Loss : 0.4891\n",
            "Epoch 347/1000 | Loss : 0.4890\n",
            "Epoch 348/1000 | Loss : 0.4888\n",
            "Epoch 349/1000 | Loss : 0.4887\n",
            "Epoch 350/1000 | Loss : 0.4885\n",
            "Epoch 351/1000 | Loss : 0.4883\n",
            "Epoch 352/1000 | Loss : 0.4882\n",
            "Epoch 353/1000 | Loss : 0.4880\n",
            "Epoch 354/1000 | Loss : 0.4879\n",
            "Epoch 355/1000 | Loss : 0.4877\n",
            "Epoch 356/1000 | Loss : 0.4875\n",
            "Epoch 357/1000 | Loss : 0.4874\n",
            "Epoch 358/1000 | Loss : 0.4872\n",
            "Epoch 359/1000 | Loss : 0.4871\n",
            "Epoch 360/1000 | Loss : 0.4869\n",
            "Epoch 361/1000 | Loss : 0.4867\n",
            "Epoch 362/1000 | Loss : 0.4866\n",
            "Epoch 363/1000 | Loss : 0.4864\n",
            "Epoch 364/1000 | Loss : 0.4863\n",
            "Epoch 365/1000 | Loss : 0.4861\n",
            "Epoch 366/1000 | Loss : 0.4860\n",
            "Epoch 367/1000 | Loss : 0.4858\n",
            "Epoch 368/1000 | Loss : 0.4856\n",
            "Epoch 369/1000 | Loss : 0.4855\n",
            "Epoch 370/1000 | Loss : 0.4853\n",
            "Epoch 371/1000 | Loss : 0.4852\n",
            "Epoch 372/1000 | Loss : 0.4850\n",
            "Epoch 373/1000 | Loss : 0.4849\n",
            "Epoch 374/1000 | Loss : 0.4847\n",
            "Epoch 375/1000 | Loss : 0.4845\n",
            "Epoch 376/1000 | Loss : 0.4844\n",
            "Epoch 377/1000 | Loss : 0.4842\n",
            "Epoch 378/1000 | Loss : 0.4841\n",
            "Epoch 379/1000 | Loss : 0.4839\n",
            "Epoch 380/1000 | Loss : 0.4838\n",
            "Epoch 381/1000 | Loss : 0.4836\n",
            "Epoch 382/1000 | Loss : 0.4834\n",
            "Epoch 383/1000 | Loss : 0.4833\n",
            "Epoch 384/1000 | Loss : 0.4831\n",
            "Epoch 385/1000 | Loss : 0.4830\n",
            "Epoch 386/1000 | Loss : 0.4828\n",
            "Epoch 387/1000 | Loss : 0.4827\n",
            "Epoch 388/1000 | Loss : 0.4825\n",
            "Epoch 389/1000 | Loss : 0.4823\n",
            "Epoch 390/1000 | Loss : 0.4822\n",
            "Epoch 391/1000 | Loss : 0.4820\n",
            "Epoch 392/1000 | Loss : 0.4819\n",
            "Epoch 393/1000 | Loss : 0.4817\n",
            "Epoch 394/1000 | Loss : 0.4816\n",
            "Epoch 395/1000 | Loss : 0.4814\n",
            "Epoch 396/1000 | Loss : 0.4813\n",
            "Epoch 397/1000 | Loss : 0.4811\n",
            "Epoch 398/1000 | Loss : 0.4809\n",
            "Epoch 399/1000 | Loss : 0.4808\n",
            "Epoch 400/1000 | Loss : 0.4806\n",
            "Epoch 401/1000 | Loss : 0.4805\n",
            "Epoch 402/1000 | Loss : 0.4803\n",
            "Epoch 403/1000 | Loss : 0.4802\n",
            "Epoch 404/1000 | Loss : 0.4800\n",
            "Epoch 405/1000 | Loss : 0.4799\n",
            "Epoch 406/1000 | Loss : 0.4797\n",
            "Epoch 407/1000 | Loss : 0.4796\n",
            "Epoch 408/1000 | Loss : 0.4794\n",
            "Epoch 409/1000 | Loss : 0.4793\n",
            "Epoch 410/1000 | Loss : 0.4791\n",
            "Epoch 411/1000 | Loss : 0.4789\n",
            "Epoch 412/1000 | Loss : 0.4788\n",
            "Epoch 413/1000 | Loss : 0.4786\n",
            "Epoch 414/1000 | Loss : 0.4785\n",
            "Epoch 415/1000 | Loss : 0.4783\n",
            "Epoch 416/1000 | Loss : 0.4782\n",
            "Epoch 417/1000 | Loss : 0.4780\n",
            "Epoch 418/1000 | Loss : 0.4779\n",
            "Epoch 419/1000 | Loss : 0.4777\n",
            "Epoch 420/1000 | Loss : 0.4776\n",
            "Epoch 421/1000 | Loss : 0.4774\n",
            "Epoch 422/1000 | Loss : 0.4773\n",
            "Epoch 423/1000 | Loss : 0.4771\n",
            "Epoch 424/1000 | Loss : 0.4770\n",
            "Epoch 425/1000 | Loss : 0.4768\n",
            "Epoch 426/1000 | Loss : 0.4767\n",
            "Epoch 427/1000 | Loss : 0.4765\n",
            "Epoch 428/1000 | Loss : 0.4763\n",
            "Epoch 429/1000 | Loss : 0.4762\n",
            "Epoch 430/1000 | Loss : 0.4760\n",
            "Epoch 431/1000 | Loss : 0.4759\n",
            "Epoch 432/1000 | Loss : 0.4757\n",
            "Epoch 433/1000 | Loss : 0.4756\n",
            "Epoch 434/1000 | Loss : 0.4754\n",
            "Epoch 435/1000 | Loss : 0.4753\n",
            "Epoch 436/1000 | Loss : 0.4751\n",
            "Epoch 437/1000 | Loss : 0.4750\n",
            "Epoch 438/1000 | Loss : 0.4748\n",
            "Epoch 439/1000 | Loss : 0.4747\n",
            "Epoch 440/1000 | Loss : 0.4745\n",
            "Epoch 441/1000 | Loss : 0.4744\n",
            "Epoch 442/1000 | Loss : 0.4742\n",
            "Epoch 443/1000 | Loss : 0.4741\n",
            "Epoch 444/1000 | Loss : 0.4739\n",
            "Epoch 445/1000 | Loss : 0.4738\n",
            "Epoch 446/1000 | Loss : 0.4736\n",
            "Epoch 447/1000 | Loss : 0.4735\n",
            "Epoch 448/1000 | Loss : 0.4733\n",
            "Epoch 449/1000 | Loss : 0.4732\n",
            "Epoch 450/1000 | Loss : 0.4730\n",
            "Epoch 451/1000 | Loss : 0.4729\n",
            "Epoch 452/1000 | Loss : 0.4727\n",
            "Epoch 453/1000 | Loss : 0.4726\n",
            "Epoch 454/1000 | Loss : 0.4724\n",
            "Epoch 455/1000 | Loss : 0.4723\n",
            "Epoch 456/1000 | Loss : 0.4721\n",
            "Epoch 457/1000 | Loss : 0.4720\n",
            "Epoch 458/1000 | Loss : 0.4718\n",
            "Epoch 459/1000 | Loss : 0.4717\n",
            "Epoch 460/1000 | Loss : 0.4715\n",
            "Epoch 461/1000 | Loss : 0.4714\n",
            "Epoch 462/1000 | Loss : 0.4712\n",
            "Epoch 463/1000 | Loss : 0.4711\n",
            "Epoch 464/1000 | Loss : 0.4710\n",
            "Epoch 465/1000 | Loss : 0.4708\n",
            "Epoch 466/1000 | Loss : 0.4707\n",
            "Epoch 467/1000 | Loss : 0.4705\n",
            "Epoch 468/1000 | Loss : 0.4704\n",
            "Epoch 469/1000 | Loss : 0.4702\n",
            "Epoch 470/1000 | Loss : 0.4701\n",
            "Epoch 471/1000 | Loss : 0.4699\n",
            "Epoch 472/1000 | Loss : 0.4698\n",
            "Epoch 473/1000 | Loss : 0.4696\n",
            "Epoch 474/1000 | Loss : 0.4695\n",
            "Epoch 475/1000 | Loss : 0.4693\n",
            "Epoch 476/1000 | Loss : 0.4692\n",
            "Epoch 477/1000 | Loss : 0.4690\n",
            "Epoch 478/1000 | Loss : 0.4689\n",
            "Epoch 479/1000 | Loss : 0.4687\n",
            "Epoch 480/1000 | Loss : 0.4686\n",
            "Epoch 481/1000 | Loss : 0.4684\n",
            "Epoch 482/1000 | Loss : 0.4683\n",
            "Epoch 483/1000 | Loss : 0.4682\n",
            "Epoch 484/1000 | Loss : 0.4680\n",
            "Epoch 485/1000 | Loss : 0.4679\n",
            "Epoch 486/1000 | Loss : 0.4677\n",
            "Epoch 487/1000 | Loss : 0.4676\n",
            "Epoch 488/1000 | Loss : 0.4674\n",
            "Epoch 489/1000 | Loss : 0.4673\n",
            "Epoch 490/1000 | Loss : 0.4671\n",
            "Epoch 491/1000 | Loss : 0.4670\n",
            "Epoch 492/1000 | Loss : 0.4668\n",
            "Epoch 493/1000 | Loss : 0.4667\n",
            "Epoch 494/1000 | Loss : 0.4665\n",
            "Epoch 495/1000 | Loss : 0.4664\n",
            "Epoch 496/1000 | Loss : 0.4663\n",
            "Epoch 497/1000 | Loss : 0.4661\n",
            "Epoch 498/1000 | Loss : 0.4660\n",
            "Epoch 499/1000 | Loss : 0.4658\n",
            "Epoch 500/1000 | Loss : 0.4657\n",
            "Epoch 501/1000 | Loss : 0.4655\n",
            "Epoch 502/1000 | Loss : 0.4654\n",
            "Epoch 503/1000 | Loss : 0.4652\n",
            "Epoch 504/1000 | Loss : 0.4651\n",
            "Epoch 505/1000 | Loss : 0.4650\n",
            "Epoch 506/1000 | Loss : 0.4648\n",
            "Epoch 507/1000 | Loss : 0.4647\n",
            "Epoch 508/1000 | Loss : 0.4645\n",
            "Epoch 509/1000 | Loss : 0.4644\n",
            "Epoch 510/1000 | Loss : 0.4642\n",
            "Epoch 511/1000 | Loss : 0.4641\n",
            "Epoch 512/1000 | Loss : 0.4640\n",
            "Epoch 513/1000 | Loss : 0.4638\n",
            "Epoch 514/1000 | Loss : 0.4637\n",
            "Epoch 515/1000 | Loss : 0.4635\n",
            "Epoch 516/1000 | Loss : 0.4634\n",
            "Epoch 517/1000 | Loss : 0.4632\n",
            "Epoch 518/1000 | Loss : 0.4631\n",
            "Epoch 519/1000 | Loss : 0.4629\n",
            "Epoch 520/1000 | Loss : 0.4628\n",
            "Epoch 521/1000 | Loss : 0.4627\n",
            "Epoch 522/1000 | Loss : 0.4625\n",
            "Epoch 523/1000 | Loss : 0.4624\n",
            "Epoch 524/1000 | Loss : 0.4622\n",
            "Epoch 525/1000 | Loss : 0.4621\n",
            "Epoch 526/1000 | Loss : 0.4619\n",
            "Epoch 527/1000 | Loss : 0.4618\n",
            "Epoch 528/1000 | Loss : 0.4617\n",
            "Epoch 529/1000 | Loss : 0.4615\n",
            "Epoch 530/1000 | Loss : 0.4614\n",
            "Epoch 531/1000 | Loss : 0.4612\n",
            "Epoch 532/1000 | Loss : 0.4611\n",
            "Epoch 533/1000 | Loss : 0.4610\n",
            "Epoch 534/1000 | Loss : 0.4608\n",
            "Epoch 535/1000 | Loss : 0.4607\n",
            "Epoch 536/1000 | Loss : 0.4605\n",
            "Epoch 537/1000 | Loss : 0.4604\n",
            "Epoch 538/1000 | Loss : 0.4602\n",
            "Epoch 539/1000 | Loss : 0.4601\n",
            "Epoch 540/1000 | Loss : 0.4600\n",
            "Epoch 541/1000 | Loss : 0.4598\n",
            "Epoch 542/1000 | Loss : 0.4597\n",
            "Epoch 543/1000 | Loss : 0.4595\n",
            "Epoch 544/1000 | Loss : 0.4594\n",
            "Epoch 545/1000 | Loss : 0.4593\n",
            "Epoch 546/1000 | Loss : 0.4591\n",
            "Epoch 547/1000 | Loss : 0.4590\n",
            "Epoch 548/1000 | Loss : 0.4588\n",
            "Epoch 549/1000 | Loss : 0.4587\n",
            "Epoch 550/1000 | Loss : 0.4586\n",
            "Epoch 551/1000 | Loss : 0.4584\n",
            "Epoch 552/1000 | Loss : 0.4583\n",
            "Epoch 553/1000 | Loss : 0.4581\n",
            "Epoch 554/1000 | Loss : 0.4580\n",
            "Epoch 555/1000 | Loss : 0.4579\n",
            "Epoch 556/1000 | Loss : 0.4577\n",
            "Epoch 557/1000 | Loss : 0.4576\n",
            "Epoch 558/1000 | Loss : 0.4574\n",
            "Epoch 559/1000 | Loss : 0.4573\n",
            "Epoch 560/1000 | Loss : 0.4572\n",
            "Epoch 561/1000 | Loss : 0.4570\n",
            "Epoch 562/1000 | Loss : 0.4569\n",
            "Epoch 563/1000 | Loss : 0.4567\n",
            "Epoch 564/1000 | Loss : 0.4566\n",
            "Epoch 565/1000 | Loss : 0.4565\n",
            "Epoch 566/1000 | Loss : 0.4563\n",
            "Epoch 567/1000 | Loss : 0.4562\n",
            "Epoch 568/1000 | Loss : 0.4561\n",
            "Epoch 569/1000 | Loss : 0.4559\n",
            "Epoch 570/1000 | Loss : 0.4558\n",
            "Epoch 571/1000 | Loss : 0.4556\n",
            "Epoch 572/1000 | Loss : 0.4555\n",
            "Epoch 573/1000 | Loss : 0.4554\n",
            "Epoch 574/1000 | Loss : 0.4552\n",
            "Epoch 575/1000 | Loss : 0.4551\n",
            "Epoch 576/1000 | Loss : 0.4550\n",
            "Epoch 577/1000 | Loss : 0.4548\n",
            "Epoch 578/1000 | Loss : 0.4547\n",
            "Epoch 579/1000 | Loss : 0.4545\n",
            "Epoch 580/1000 | Loss : 0.4544\n",
            "Epoch 581/1000 | Loss : 0.4543\n",
            "Epoch 582/1000 | Loss : 0.4541\n",
            "Epoch 583/1000 | Loss : 0.4540\n",
            "Epoch 584/1000 | Loss : 0.4539\n",
            "Epoch 585/1000 | Loss : 0.4537\n",
            "Epoch 586/1000 | Loss : 0.4536\n",
            "Epoch 587/1000 | Loss : 0.4534\n",
            "Epoch 588/1000 | Loss : 0.4533\n",
            "Epoch 589/1000 | Loss : 0.4532\n",
            "Epoch 590/1000 | Loss : 0.4530\n",
            "Epoch 591/1000 | Loss : 0.4529\n",
            "Epoch 592/1000 | Loss : 0.4528\n",
            "Epoch 593/1000 | Loss : 0.4526\n",
            "Epoch 594/1000 | Loss : 0.4525\n",
            "Epoch 595/1000 | Loss : 0.4523\n",
            "Epoch 596/1000 | Loss : 0.4522\n",
            "Epoch 597/1000 | Loss : 0.4521\n",
            "Epoch 598/1000 | Loss : 0.4519\n",
            "Epoch 599/1000 | Loss : 0.4518\n",
            "Epoch 600/1000 | Loss : 0.4517\n",
            "Epoch 601/1000 | Loss : 0.4515\n",
            "Epoch 602/1000 | Loss : 0.4514\n",
            "Epoch 603/1000 | Loss : 0.4513\n",
            "Epoch 604/1000 | Loss : 0.4511\n",
            "Epoch 605/1000 | Loss : 0.4510\n",
            "Epoch 606/1000 | Loss : 0.4509\n",
            "Epoch 607/1000 | Loss : 0.4507\n",
            "Epoch 608/1000 | Loss : 0.4506\n",
            "Epoch 609/1000 | Loss : 0.4505\n",
            "Epoch 610/1000 | Loss : 0.4503\n",
            "Epoch 611/1000 | Loss : 0.4502\n",
            "Epoch 612/1000 | Loss : 0.4500\n",
            "Epoch 613/1000 | Loss : 0.4499\n",
            "Epoch 614/1000 | Loss : 0.4498\n",
            "Epoch 615/1000 | Loss : 0.4496\n",
            "Epoch 616/1000 | Loss : 0.4495\n",
            "Epoch 617/1000 | Loss : 0.4494\n",
            "Epoch 618/1000 | Loss : 0.4492\n",
            "Epoch 619/1000 | Loss : 0.4491\n",
            "Epoch 620/1000 | Loss : 0.4490\n",
            "Epoch 621/1000 | Loss : 0.4488\n",
            "Epoch 622/1000 | Loss : 0.4487\n",
            "Epoch 623/1000 | Loss : 0.4486\n",
            "Epoch 624/1000 | Loss : 0.4484\n",
            "Epoch 625/1000 | Loss : 0.4483\n",
            "Epoch 626/1000 | Loss : 0.4482\n",
            "Epoch 627/1000 | Loss : 0.4480\n",
            "Epoch 628/1000 | Loss : 0.4479\n",
            "Epoch 629/1000 | Loss : 0.4478\n",
            "Epoch 630/1000 | Loss : 0.4476\n",
            "Epoch 631/1000 | Loss : 0.4475\n",
            "Epoch 632/1000 | Loss : 0.4474\n",
            "Epoch 633/1000 | Loss : 0.4472\n",
            "Epoch 634/1000 | Loss : 0.4471\n",
            "Epoch 635/1000 | Loss : 0.4470\n",
            "Epoch 636/1000 | Loss : 0.4468\n",
            "Epoch 637/1000 | Loss : 0.4467\n",
            "Epoch 638/1000 | Loss : 0.4466\n",
            "Epoch 639/1000 | Loss : 0.4464\n",
            "Epoch 640/1000 | Loss : 0.4463\n",
            "Epoch 641/1000 | Loss : 0.4462\n",
            "Epoch 642/1000 | Loss : 0.4461\n",
            "Epoch 643/1000 | Loss : 0.4459\n",
            "Epoch 644/1000 | Loss : 0.4458\n",
            "Epoch 645/1000 | Loss : 0.4457\n",
            "Epoch 646/1000 | Loss : 0.4455\n",
            "Epoch 647/1000 | Loss : 0.4454\n",
            "Epoch 648/1000 | Loss : 0.4453\n",
            "Epoch 649/1000 | Loss : 0.4451\n",
            "Epoch 650/1000 | Loss : 0.4450\n",
            "Epoch 651/1000 | Loss : 0.4449\n",
            "Epoch 652/1000 | Loss : 0.4447\n",
            "Epoch 653/1000 | Loss : 0.4446\n",
            "Epoch 654/1000 | Loss : 0.4445\n",
            "Epoch 655/1000 | Loss : 0.4443\n",
            "Epoch 656/1000 | Loss : 0.4442\n",
            "Epoch 657/1000 | Loss : 0.4441\n",
            "Epoch 658/1000 | Loss : 0.4439\n",
            "Epoch 659/1000 | Loss : 0.4438\n",
            "Epoch 660/1000 | Loss : 0.4437\n",
            "Epoch 661/1000 | Loss : 0.4436\n",
            "Epoch 662/1000 | Loss : 0.4434\n",
            "Epoch 663/1000 | Loss : 0.4433\n",
            "Epoch 664/1000 | Loss : 0.4432\n",
            "Epoch 665/1000 | Loss : 0.4430\n",
            "Epoch 666/1000 | Loss : 0.4429\n",
            "Epoch 667/1000 | Loss : 0.4428\n",
            "Epoch 668/1000 | Loss : 0.4426\n",
            "Epoch 669/1000 | Loss : 0.4425\n",
            "Epoch 670/1000 | Loss : 0.4424\n",
            "Epoch 671/1000 | Loss : 0.4423\n",
            "Epoch 672/1000 | Loss : 0.4421\n",
            "Epoch 673/1000 | Loss : 0.4420\n",
            "Epoch 674/1000 | Loss : 0.4419\n",
            "Epoch 675/1000 | Loss : 0.4417\n",
            "Epoch 676/1000 | Loss : 0.4416\n",
            "Epoch 677/1000 | Loss : 0.4415\n",
            "Epoch 678/1000 | Loss : 0.4414\n",
            "Epoch 679/1000 | Loss : 0.4412\n",
            "Epoch 680/1000 | Loss : 0.4411\n",
            "Epoch 681/1000 | Loss : 0.4410\n",
            "Epoch 682/1000 | Loss : 0.4408\n",
            "Epoch 683/1000 | Loss : 0.4407\n",
            "Epoch 684/1000 | Loss : 0.4406\n",
            "Epoch 685/1000 | Loss : 0.4405\n",
            "Epoch 686/1000 | Loss : 0.4403\n",
            "Epoch 687/1000 | Loss : 0.4402\n",
            "Epoch 688/1000 | Loss : 0.4401\n",
            "Epoch 689/1000 | Loss : 0.4399\n",
            "Epoch 690/1000 | Loss : 0.4398\n",
            "Epoch 691/1000 | Loss : 0.4397\n",
            "Epoch 692/1000 | Loss : 0.4396\n",
            "Epoch 693/1000 | Loss : 0.4394\n",
            "Epoch 694/1000 | Loss : 0.4393\n",
            "Epoch 695/1000 | Loss : 0.4392\n",
            "Epoch 696/1000 | Loss : 0.4390\n",
            "Epoch 697/1000 | Loss : 0.4389\n",
            "Epoch 698/1000 | Loss : 0.4388\n",
            "Epoch 699/1000 | Loss : 0.4387\n",
            "Epoch 700/1000 | Loss : 0.4385\n",
            "Epoch 701/1000 | Loss : 0.4384\n",
            "Epoch 702/1000 | Loss : 0.4383\n",
            "Epoch 703/1000 | Loss : 0.4382\n",
            "Epoch 704/1000 | Loss : 0.4380\n",
            "Epoch 705/1000 | Loss : 0.4379\n",
            "Epoch 706/1000 | Loss : 0.4378\n",
            "Epoch 707/1000 | Loss : 0.4376\n",
            "Epoch 708/1000 | Loss : 0.4375\n",
            "Epoch 709/1000 | Loss : 0.4374\n",
            "Epoch 710/1000 | Loss : 0.4373\n",
            "Epoch 711/1000 | Loss : 0.4371\n",
            "Epoch 712/1000 | Loss : 0.4370\n",
            "Epoch 713/1000 | Loss : 0.4369\n",
            "Epoch 714/1000 | Loss : 0.4368\n",
            "Epoch 715/1000 | Loss : 0.4366\n",
            "Epoch 716/1000 | Loss : 0.4365\n",
            "Epoch 717/1000 | Loss : 0.4364\n",
            "Epoch 718/1000 | Loss : 0.4363\n",
            "Epoch 719/1000 | Loss : 0.4361\n",
            "Epoch 720/1000 | Loss : 0.4360\n",
            "Epoch 721/1000 | Loss : 0.4359\n",
            "Epoch 722/1000 | Loss : 0.4358\n",
            "Epoch 723/1000 | Loss : 0.4356\n",
            "Epoch 724/1000 | Loss : 0.4355\n",
            "Epoch 725/1000 | Loss : 0.4354\n",
            "Epoch 726/1000 | Loss : 0.4353\n",
            "Epoch 727/1000 | Loss : 0.4351\n",
            "Epoch 728/1000 | Loss : 0.4350\n",
            "Epoch 729/1000 | Loss : 0.4349\n",
            "Epoch 730/1000 | Loss : 0.4348\n",
            "Epoch 731/1000 | Loss : 0.4346\n",
            "Epoch 732/1000 | Loss : 0.4345\n",
            "Epoch 733/1000 | Loss : 0.4344\n",
            "Epoch 734/1000 | Loss : 0.4343\n",
            "Epoch 735/1000 | Loss : 0.4341\n",
            "Epoch 736/1000 | Loss : 0.4340\n",
            "Epoch 737/1000 | Loss : 0.4339\n",
            "Epoch 738/1000 | Loss : 0.4338\n",
            "Epoch 739/1000 | Loss : 0.4336\n",
            "Epoch 740/1000 | Loss : 0.4335\n",
            "Epoch 741/1000 | Loss : 0.4334\n",
            "Epoch 742/1000 | Loss : 0.4333\n",
            "Epoch 743/1000 | Loss : 0.4331\n",
            "Epoch 744/1000 | Loss : 0.4330\n",
            "Epoch 745/1000 | Loss : 0.4329\n",
            "Epoch 746/1000 | Loss : 0.4328\n",
            "Epoch 747/1000 | Loss : 0.4326\n",
            "Epoch 748/1000 | Loss : 0.4325\n",
            "Epoch 749/1000 | Loss : 0.4324\n",
            "Epoch 750/1000 | Loss : 0.4323\n",
            "Epoch 751/1000 | Loss : 0.4321\n",
            "Epoch 752/1000 | Loss : 0.4320\n",
            "Epoch 753/1000 | Loss : 0.4319\n",
            "Epoch 754/1000 | Loss : 0.4318\n",
            "Epoch 755/1000 | Loss : 0.4317\n",
            "Epoch 756/1000 | Loss : 0.4315\n",
            "Epoch 757/1000 | Loss : 0.4314\n",
            "Epoch 758/1000 | Loss : 0.4313\n",
            "Epoch 759/1000 | Loss : 0.4312\n",
            "Epoch 760/1000 | Loss : 0.4310\n",
            "Epoch 761/1000 | Loss : 0.4309\n",
            "Epoch 762/1000 | Loss : 0.4308\n",
            "Epoch 763/1000 | Loss : 0.4307\n",
            "Epoch 764/1000 | Loss : 0.4306\n",
            "Epoch 765/1000 | Loss : 0.4304\n",
            "Epoch 766/1000 | Loss : 0.4303\n",
            "Epoch 767/1000 | Loss : 0.4302\n",
            "Epoch 768/1000 | Loss : 0.4301\n",
            "Epoch 769/1000 | Loss : 0.4299\n",
            "Epoch 770/1000 | Loss : 0.4298\n",
            "Epoch 771/1000 | Loss : 0.4297\n",
            "Epoch 772/1000 | Loss : 0.4296\n",
            "Epoch 773/1000 | Loss : 0.4295\n",
            "Epoch 774/1000 | Loss : 0.4293\n",
            "Epoch 775/1000 | Loss : 0.4292\n",
            "Epoch 776/1000 | Loss : 0.4291\n",
            "Epoch 777/1000 | Loss : 0.4290\n",
            "Epoch 778/1000 | Loss : 0.4288\n",
            "Epoch 779/1000 | Loss : 0.4287\n",
            "Epoch 780/1000 | Loss : 0.4286\n",
            "Epoch 781/1000 | Loss : 0.4285\n",
            "Epoch 782/1000 | Loss : 0.4284\n",
            "Epoch 783/1000 | Loss : 0.4282\n",
            "Epoch 784/1000 | Loss : 0.4281\n",
            "Epoch 785/1000 | Loss : 0.4280\n",
            "Epoch 786/1000 | Loss : 0.4279\n",
            "Epoch 787/1000 | Loss : 0.4278\n",
            "Epoch 788/1000 | Loss : 0.4276\n",
            "Epoch 789/1000 | Loss : 0.4275\n",
            "Epoch 790/1000 | Loss : 0.4274\n",
            "Epoch 791/1000 | Loss : 0.4273\n",
            "Epoch 792/1000 | Loss : 0.4272\n",
            "Epoch 793/1000 | Loss : 0.4270\n",
            "Epoch 794/1000 | Loss : 0.4269\n",
            "Epoch 795/1000 | Loss : 0.4268\n",
            "Epoch 796/1000 | Loss : 0.4267\n",
            "Epoch 797/1000 | Loss : 0.4266\n",
            "Epoch 798/1000 | Loss : 0.4264\n",
            "Epoch 799/1000 | Loss : 0.4263\n",
            "Epoch 800/1000 | Loss : 0.4262\n",
            "Epoch 801/1000 | Loss : 0.4261\n",
            "Epoch 802/1000 | Loss : 0.4260\n",
            "Epoch 803/1000 | Loss : 0.4258\n",
            "Epoch 804/1000 | Loss : 0.4257\n",
            "Epoch 805/1000 | Loss : 0.4256\n",
            "Epoch 806/1000 | Loss : 0.4255\n",
            "Epoch 807/1000 | Loss : 0.4254\n",
            "Epoch 808/1000 | Loss : 0.4252\n",
            "Epoch 809/1000 | Loss : 0.4251\n",
            "Epoch 810/1000 | Loss : 0.4250\n",
            "Epoch 811/1000 | Loss : 0.4249\n",
            "Epoch 812/1000 | Loss : 0.4248\n",
            "Epoch 813/1000 | Loss : 0.4247\n",
            "Epoch 814/1000 | Loss : 0.4245\n",
            "Epoch 815/1000 | Loss : 0.4244\n",
            "Epoch 816/1000 | Loss : 0.4243\n",
            "Epoch 817/1000 | Loss : 0.4242\n",
            "Epoch 818/1000 | Loss : 0.4241\n",
            "Epoch 819/1000 | Loss : 0.4239\n",
            "Epoch 820/1000 | Loss : 0.4238\n",
            "Epoch 821/1000 | Loss : 0.4237\n",
            "Epoch 822/1000 | Loss : 0.4236\n",
            "Epoch 823/1000 | Loss : 0.4235\n",
            "Epoch 824/1000 | Loss : 0.4234\n",
            "Epoch 825/1000 | Loss : 0.4232\n",
            "Epoch 826/1000 | Loss : 0.4231\n",
            "Epoch 827/1000 | Loss : 0.4230\n",
            "Epoch 828/1000 | Loss : 0.4229\n",
            "Epoch 829/1000 | Loss : 0.4228\n",
            "Epoch 830/1000 | Loss : 0.4226\n",
            "Epoch 831/1000 | Loss : 0.4225\n",
            "Epoch 832/1000 | Loss : 0.4224\n",
            "Epoch 833/1000 | Loss : 0.4223\n",
            "Epoch 834/1000 | Loss : 0.4222\n",
            "Epoch 835/1000 | Loss : 0.4221\n",
            "Epoch 836/1000 | Loss : 0.4219\n",
            "Epoch 837/1000 | Loss : 0.4218\n",
            "Epoch 838/1000 | Loss : 0.4217\n",
            "Epoch 839/1000 | Loss : 0.4216\n",
            "Epoch 840/1000 | Loss : 0.4215\n",
            "Epoch 841/1000 | Loss : 0.4214\n",
            "Epoch 842/1000 | Loss : 0.4212\n",
            "Epoch 843/1000 | Loss : 0.4211\n",
            "Epoch 844/1000 | Loss : 0.4210\n",
            "Epoch 845/1000 | Loss : 0.4209\n",
            "Epoch 846/1000 | Loss : 0.4208\n",
            "Epoch 847/1000 | Loss : 0.4207\n",
            "Epoch 848/1000 | Loss : 0.4205\n",
            "Epoch 849/1000 | Loss : 0.4204\n",
            "Epoch 850/1000 | Loss : 0.4203\n",
            "Epoch 851/1000 | Loss : 0.4202\n",
            "Epoch 852/1000 | Loss : 0.4201\n",
            "Epoch 853/1000 | Loss : 0.4200\n",
            "Epoch 854/1000 | Loss : 0.4199\n",
            "Epoch 855/1000 | Loss : 0.4197\n",
            "Epoch 856/1000 | Loss : 0.4196\n",
            "Epoch 857/1000 | Loss : 0.4195\n",
            "Epoch 858/1000 | Loss : 0.4194\n",
            "Epoch 859/1000 | Loss : 0.4193\n",
            "Epoch 860/1000 | Loss : 0.4192\n",
            "Epoch 861/1000 | Loss : 0.4190\n",
            "Epoch 862/1000 | Loss : 0.4189\n",
            "Epoch 863/1000 | Loss : 0.4188\n",
            "Epoch 864/1000 | Loss : 0.4187\n",
            "Epoch 865/1000 | Loss : 0.4186\n",
            "Epoch 866/1000 | Loss : 0.4185\n",
            "Epoch 867/1000 | Loss : 0.4184\n",
            "Epoch 868/1000 | Loss : 0.4182\n",
            "Epoch 869/1000 | Loss : 0.4181\n",
            "Epoch 870/1000 | Loss : 0.4180\n",
            "Epoch 871/1000 | Loss : 0.4179\n",
            "Epoch 872/1000 | Loss : 0.4178\n",
            "Epoch 873/1000 | Loss : 0.4177\n",
            "Epoch 874/1000 | Loss : 0.4176\n",
            "Epoch 875/1000 | Loss : 0.4174\n",
            "Epoch 876/1000 | Loss : 0.4173\n",
            "Epoch 877/1000 | Loss : 0.4172\n",
            "Epoch 878/1000 | Loss : 0.4171\n",
            "Epoch 879/1000 | Loss : 0.4170\n",
            "Epoch 880/1000 | Loss : 0.4169\n",
            "Epoch 881/1000 | Loss : 0.4168\n",
            "Epoch 882/1000 | Loss : 0.4166\n",
            "Epoch 883/1000 | Loss : 0.4165\n",
            "Epoch 884/1000 | Loss : 0.4164\n",
            "Epoch 885/1000 | Loss : 0.4163\n",
            "Epoch 886/1000 | Loss : 0.4162\n",
            "Epoch 887/1000 | Loss : 0.4161\n",
            "Epoch 888/1000 | Loss : 0.4160\n",
            "Epoch 889/1000 | Loss : 0.4158\n",
            "Epoch 890/1000 | Loss : 0.4157\n",
            "Epoch 891/1000 | Loss : 0.4156\n",
            "Epoch 892/1000 | Loss : 0.4155\n",
            "Epoch 893/1000 | Loss : 0.4154\n",
            "Epoch 894/1000 | Loss : 0.4153\n",
            "Epoch 895/1000 | Loss : 0.4152\n",
            "Epoch 896/1000 | Loss : 0.4151\n",
            "Epoch 897/1000 | Loss : 0.4149\n",
            "Epoch 898/1000 | Loss : 0.4148\n",
            "Epoch 899/1000 | Loss : 0.4147\n",
            "Epoch 900/1000 | Loss : 0.4146\n",
            "Epoch 901/1000 | Loss : 0.4145\n",
            "Epoch 902/1000 | Loss : 0.4144\n",
            "Epoch 903/1000 | Loss : 0.4143\n",
            "Epoch 904/1000 | Loss : 0.4142\n",
            "Epoch 905/1000 | Loss : 0.4140\n",
            "Epoch 906/1000 | Loss : 0.4139\n",
            "Epoch 907/1000 | Loss : 0.4138\n",
            "Epoch 908/1000 | Loss : 0.4137\n",
            "Epoch 909/1000 | Loss : 0.4136\n",
            "Epoch 910/1000 | Loss : 0.4135\n",
            "Epoch 911/1000 | Loss : 0.4134\n",
            "Epoch 912/1000 | Loss : 0.4133\n",
            "Epoch 913/1000 | Loss : 0.4132\n",
            "Epoch 914/1000 | Loss : 0.4130\n",
            "Epoch 915/1000 | Loss : 0.4129\n",
            "Epoch 916/1000 | Loss : 0.4128\n",
            "Epoch 917/1000 | Loss : 0.4127\n",
            "Epoch 918/1000 | Loss : 0.4126\n",
            "Epoch 919/1000 | Loss : 0.4125\n",
            "Epoch 920/1000 | Loss : 0.4124\n",
            "Epoch 921/1000 | Loss : 0.4123\n",
            "Epoch 922/1000 | Loss : 0.4122\n",
            "Epoch 923/1000 | Loss : 0.4120\n",
            "Epoch 924/1000 | Loss : 0.4119\n",
            "Epoch 925/1000 | Loss : 0.4118\n",
            "Epoch 926/1000 | Loss : 0.4117\n",
            "Epoch 927/1000 | Loss : 0.4116\n",
            "Epoch 928/1000 | Loss : 0.4115\n",
            "Epoch 929/1000 | Loss : 0.4114\n",
            "Epoch 930/1000 | Loss : 0.4113\n",
            "Epoch 931/1000 | Loss : 0.4112\n",
            "Epoch 932/1000 | Loss : 0.4110\n",
            "Epoch 933/1000 | Loss : 0.4109\n",
            "Epoch 934/1000 | Loss : 0.4108\n",
            "Epoch 935/1000 | Loss : 0.4107\n",
            "Epoch 936/1000 | Loss : 0.4106\n",
            "Epoch 937/1000 | Loss : 0.4105\n",
            "Epoch 938/1000 | Loss : 0.4104\n",
            "Epoch 939/1000 | Loss : 0.4103\n",
            "Epoch 940/1000 | Loss : 0.4102\n",
            "Epoch 941/1000 | Loss : 0.4101\n",
            "Epoch 942/1000 | Loss : 0.4099\n",
            "Epoch 943/1000 | Loss : 0.4098\n",
            "Epoch 944/1000 | Loss : 0.4097\n",
            "Epoch 945/1000 | Loss : 0.4096\n",
            "Epoch 946/1000 | Loss : 0.4095\n",
            "Epoch 947/1000 | Loss : 0.4094\n",
            "Epoch 948/1000 | Loss : 0.4093\n",
            "Epoch 949/1000 | Loss : 0.4092\n",
            "Epoch 950/1000 | Loss : 0.4091\n",
            "Epoch 951/1000 | Loss : 0.4090\n",
            "Epoch 952/1000 | Loss : 0.4089\n",
            "Epoch 953/1000 | Loss : 0.4087\n",
            "Epoch 954/1000 | Loss : 0.4086\n",
            "Epoch 955/1000 | Loss : 0.4085\n",
            "Epoch 956/1000 | Loss : 0.4084\n",
            "Epoch 957/1000 | Loss : 0.4083\n",
            "Epoch 958/1000 | Loss : 0.4082\n",
            "Epoch 959/1000 | Loss : 0.4081\n",
            "Epoch 960/1000 | Loss : 0.4080\n",
            "Epoch 961/1000 | Loss : 0.4079\n",
            "Epoch 962/1000 | Loss : 0.4078\n",
            "Epoch 963/1000 | Loss : 0.4077\n",
            "Epoch 964/1000 | Loss : 0.4076\n",
            "Epoch 965/1000 | Loss : 0.4074\n",
            "Epoch 966/1000 | Loss : 0.4073\n",
            "Epoch 967/1000 | Loss : 0.4072\n",
            "Epoch 968/1000 | Loss : 0.4071\n",
            "Epoch 969/1000 | Loss : 0.4070\n",
            "Epoch 970/1000 | Loss : 0.4069\n",
            "Epoch 971/1000 | Loss : 0.4068\n",
            "Epoch 972/1000 | Loss : 0.4067\n",
            "Epoch 973/1000 | Loss : 0.4066\n",
            "Epoch 974/1000 | Loss : 0.4065\n",
            "Epoch 975/1000 | Loss : 0.4064\n",
            "Epoch 976/1000 | Loss : 0.4063\n",
            "Epoch 977/1000 | Loss : 0.4061\n",
            "Epoch 978/1000 | Loss : 0.4060\n",
            "Epoch 979/1000 | Loss : 0.4059\n",
            "Epoch 980/1000 | Loss : 0.4058\n",
            "Epoch 981/1000 | Loss : 0.4057\n",
            "Epoch 982/1000 | Loss : 0.4056\n",
            "Epoch 983/1000 | Loss : 0.4055\n",
            "Epoch 984/1000 | Loss : 0.4054\n",
            "Epoch 985/1000 | Loss : 0.4053\n",
            "Epoch 986/1000 | Loss : 0.4052\n",
            "Epoch 987/1000 | Loss : 0.4051\n",
            "Epoch 988/1000 | Loss : 0.4050\n",
            "Epoch 989/1000 | Loss : 0.4049\n",
            "Epoch 990/1000 | Loss : 0.4048\n",
            "Epoch 991/1000 | Loss : 0.4047\n",
            "Epoch 992/1000 | Loss : 0.4045\n",
            "Epoch 993/1000 | Loss : 0.4044\n",
            "Epoch 994/1000 | Loss : 0.4043\n",
            "Epoch 995/1000 | Loss : 0.4042\n",
            "Epoch 996/1000 | Loss : 0.4041\n",
            "Epoch 997/1000 | Loss : 0.4040\n",
            "Epoch 998/1000 | Loss : 0.4039\n",
            "Epoch 999/1000 | Loss : 0.4038\n",
            "Epoch 1000/1000 | Loss : 0.4037\n",
            "\n",
            "Let's predict the hours need to score above 50%\n",
            "==================================================\n",
            "Prediction after 1 hour of training: 0.3166 | Above 50%: False\n",
            "Prediction after 1 hour of training: 0.9852 | Above 50%: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deglipzNZQ-j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}